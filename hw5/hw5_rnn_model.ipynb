{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN based word-level language model for the Shakespeare plays and poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTLK version 3.2.3\n",
      "TensorFlow version 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "print('NTLK version', nltk.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adubitskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_plays.pickle', 'rb') as handle:\n",
    "    speeches = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_poetries.pickle', 'rb') as handle:\n",
    "    poetries = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare_texts = [s['speech_text'] for s in speeches] + [p['text'] for p in poetries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sample (first ten speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In delivering my son from me, I bury a second husband.', \"And I in going, madam, weep o'er my father's death\\nanew: but I must attend his majesty's command, to\\nwhom I am now in ward, evermore in subjection.\", 'You shall find of the king a husband, madam; you,\\nsir, a father: he that so generally is at all times\\ngood must of necessity hold his virtue to you; whose\\nworthiness would stir it up where it wanted rather\\nthan lack it where there is such abundance.', \"What hope is there of his majesty's amendment?\", 'He hath abandoned his physicians, madam; under whose\\npractises he hath persecuted time with hope, and\\nfinds no other advantage in the process but only the\\nlosing of hope by time.', \"This young gentlewoman had a father,--O, that\\n'had'! how sad a passage 'tis!--whose skill was\\nalmost as great as his honesty; had it stretched so\\nfar, would have made nature immortal, and death\\nshould have play for lack of work. Would, for the\\nking's sake, he were living! I think it would be\\nthe death of the king's disease.\", 'How called you the man you speak of, madam?', 'He was famous, sir, in his profession, and it was\\nhis great right to be so: Gerard de Narbon.', 'He was excellent indeed, madam: the king very\\nlately spoke of him admiringly and mourningly: he\\nwas skilful enough to have lived still, if knowledge\\ncould be set up against mortality.', 'What is it, my good lord, the king languishes of?']\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text in self.texts:\n",
    "            for s in self.tokenizer.tokenize(text.lower()):\n",
    "                for p in s.splitlines():\n",
    "                    yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in delivering my son from me, i bury a second husband.\n",
      "----------------------------------------\n",
      "and i in going, madam, weep o'er my father's death\n",
      "----------------------------------------\n",
      "anew: but i must attend his majesty's command, to\n",
      "----------------------------------------\n",
      "whom i am now in ward, evermore in subjection.\n",
      "----------------------------------------\n",
      "you shall find of the king a husband, madam; you,\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(SentenceGenerator(shakespeare_texts), 5):\n",
    "    print(s)\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordGenerator(object):\n",
    "    \n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.trans = str.maketrans('','', string.punctuation)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for s in SentenceGenerator(self.texts):\n",
    "            for w in s.translate(self.trans).split():\n",
    "                yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "delivering\n",
      "my\n",
      "son\n",
      "from\n",
      "me\n",
      "i\n",
      "bury\n",
      "a\n",
      "second\n",
      "husband\n",
      "and\n",
      "i\n",
      "in\n",
      "going\n",
      "madam\n",
      "weep\n",
      "oer\n",
      "my\n",
      "fathers\n"
     ]
    }
   ],
   "source": [
    "for w in itertools.islice(WordGenerator(shakespeare_texts), 20):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(texts):\n",
    "    wordGen = WordGenerator(texts)\n",
    "    counter = collections.Counter(wordGen)\n",
    "    # unique list of words with the frequencies\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # unique list of words\n",
    "    words, x = list(zip(*count_pairs))\n",
    "    # reserve 0 for padding, 1 for out of vocabulary\n",
    "    start_index = 2\n",
    "    return words, dict(zip(words, range(start_index, len(words) + start_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare vocabulary size: 27042\n"
     ]
    }
   ],
   "source": [
    "shakespeare_words, shakespeare_vocabulary = build_vocabulary(shakespeare_texts)\n",
    "print('Shakespeare vocabulary size:', len(shakespeare_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we could translate back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7114 transport\n"
     ]
    }
   ],
   "source": [
    "i = shakespeare_vocabulary['transport']\n",
    "print(i, shakespeare_words[i- 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates sentences using the words Ids\n",
    "class EmbeddedSentenceGenerator(object):\n",
    "\n",
    "    def __init__(self, texts, vocabulary):\n",
    "        self.texts = texts\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        trans = str.maketrans('','', string.punctuation)\n",
    "\n",
    "        for s in SentenceGenerator(self.texts):\n",
    "            yield [ self.vocabulary[w] if w in self.vocabulary else 1 for w in s.translate(trans).split()]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8418, 9, 186, 48, 15, 4, 1908, 7, 716, 296]\n",
      "----------------------------------------\n",
      "[3, 4, 11, 793, 190, 524, 420, 9, 366, 128]\n",
      "----------------------------------------\n",
      "[5067, 23, 4, 88, 675, 18, 4610, 561, 5]\n",
      "----------------------------------------\n",
      "[226, 4, 64, 45, 11, 3994, 2499, 11, 6380]\n",
      "----------------------------------------\n",
      "[8, 39, 188, 6, 2, 107, 7, 296, 190, 8]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary), 5):\n",
    "    print(s)\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we could recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'delivering', 'my', 'son', 'from', 'me', 'i', 'bury', 'a', 'second', 'husband']\n",
      "----------------------------------------\n",
      "['and', 'i', 'in', 'going', 'madam', 'weep', 'oer', 'my', 'fathers', 'death']\n",
      "----------------------------------------\n",
      "['anew', 'but', 'i', 'must', 'attend', 'his', 'majestys', 'command', 'to']\n",
      "----------------------------------------\n",
      "['whom', 'i', 'am', 'now', 'in', 'ward', 'evermore', 'in', 'subjection']\n",
      "----------------------------------------\n",
      "['you', 'shall', 'find', 'of', 'the', 'king', 'a', 'husband', 'madam', 'you']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary), 5):\n",
    "    print([shakespeare_words[i-2] for i in s])\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# factory for creating the epochs\n",
    "class EpochFactory(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.raw_data = [word for sentence in sentences for word in sentence]\n",
    "\n",
    "    def epoch(self, batch_size, time_steps):\n",
    "        return Epoch(self.raw_data, batch_size, time_steps)\n",
    "    \n",
    "# provides one epoch worth of data \n",
    "class Epoch(object):\n",
    "    def __init__(self, words, batch_size, time_steps):\n",
    "        self.raw_data = words\n",
    "        self.batch_size = batch_size\n",
    "        self.time_steps = time_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "\n",
    "        data_len = np.size(self.raw_data)\n",
    "        batch_len = data_len // self.batch_size\n",
    "        chunk_len = (batch_len - 1) // self.time_steps\n",
    "\n",
    "        assert (chunk_len > 0), \"chunk_len == 0, decrease batch_size or num_steps\"\n",
    "\n",
    "        data = np.reshape(self.raw_data[0 : self.batch_size * batch_len], [self.batch_size, batch_len])\n",
    "        \n",
    "        for i in range(chunk_len):\n",
    "            x = data[0 : self.batch_size , i * self.time_steps     : (i + 1) * self.time_steps] \n",
    "            y = data[0 : self.batch_size , i * self.time_steps + 1 : (i + 1) * self.time_steps + 1]\n",
    "            yield x, y, float(i)/float(chunk_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 116152 sentences\n"
     ]
    }
   ],
   "source": [
    "shakespeare_sentences = [s for s in EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary)]\n",
    "print('Total %d sentences'%len(shakespeare_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fact = EpochFactory(shakespeare_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Inputs----------------\n",
      "[11, 8418, 9, 186, 48, 15, 4, 1908, 7, 716, 296, 3]\n",
      "[938, 5, 21340, 53, 37, 14, 83, 19, 1280, 47, 2, 1456]\n",
      "[8, 38, 75, 78, 731, 140, 37, 615, 2148, 19, 31, 140]\n",
      "[12, 832, 237, 1294, 10, 4, 905, 3, 26418, 11, 7, 627]\n",
      "[7, 3473, 63, 262, 8, 1469, 13753, 15853, 2357, 9970, 13396, 7688]\n",
      "[6, 107, 1180, 14837, 1438, 5, 107, 664, 3119, 65, 4, 59]\n",
      "[28, 23, 30, 3655, 334, 8, 5, 24, 16, 86, 29, 8]\n",
      "[26, 12081, 7, 1424, 7, 393, 7, 509, 17, 173, 32, 134]\n",
      "---------------Targets---------------\n",
      "[8418, 9, 186, 48, 15, 4, 1908, 7, 716, 296, 3, 4]\n",
      "[5, 21340, 53, 37, 14, 83, 19, 1280, 47, 2, 1456, 124]\n",
      "[38, 75, 78, 731, 140, 37, 615, 2148, 19, 31, 140, 46]\n",
      "[832, 237, 1294, 10, 4, 905, 3, 26418, 11, 7, 627, 25]\n",
      "[3473, 63, 262, 8, 1469, 13753, 15853, 2357, 9970, 13396, 7688, 14624]\n",
      "[107, 1180, 14837, 1438, 5, 107, 664, 3119, 65, 4, 59, 756]\n",
      "[23, 30, 3655, 334, 8, 5, 24, 16, 86, 29, 8, 19]\n",
      "[12081, 7, 1424, 7, 393, 7, 509, 17, 173, 32, 134, 50]\n"
     ]
    }
   ],
   "source": [
    "epoch = fact.epoch(batch_size = 8, time_steps = 12)\n",
    "for x, y, progress in itertools.islice(epoch, 1):\n",
    "    print('---------------Inputs----------------')\n",
    "    for s in x:\n",
    "        print([i for i in s])\n",
    "    print('---------------Targets---------------')\n",
    "    for s in y:\n",
    "        print([i for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample chunk recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Inputs----------------\n",
      "['in', 'delivering', 'my', 'son', 'from', 'me', 'i', 'bury', 'a', 'second', 'husband', 'and']\n",
      "['int', 'to', 'mete', 'at', 'if', 'it', 'may', 'be', 'wide', 'o', 'the', 'bow']\n",
      "['you', 'are', 'like', 'an', 'honourable', 'father', 'if', 'signior', 'leonato', 'be', 'her', 'father']\n",
      "['is', 'fine', 'full', 'perfect', 'that', 'i', 'taste', 'and', 'violenteth', 'in', 'a', 'sense']\n",
      "['a', 'bachelor', 'how', 'answer', 'you', 'la', 'plus', 'belle', 'katharine', 'du', 'monde', 'mon']\n",
      "['of', 'king', 'henry', 'vi', 'ghost', 'to', 'king', 'richard', 'iii', 'when', 'i', 'was']\n",
      "['so', 'but', 'what', 'compact', 'mean', 'you', 'to', 'have', 'with', 'us', 'will', 'you']\n",
      "['thou', 'counterfeitst', 'a', 'bark', 'a', 'sea', 'a', 'wind', 'for', 'still', 'thy', 'eyes']\n",
      "---------------Targets---------------\n",
      "['delivering', 'my', 'son', 'from', 'me', 'i', 'bury', 'a', 'second', 'husband', 'and', 'i']\n",
      "['to', 'mete', 'at', 'if', 'it', 'may', 'be', 'wide', 'o', 'the', 'bow', 'hand']\n",
      "['are', 'like', 'an', 'honourable', 'father', 'if', 'signior', 'leonato', 'be', 'her', 'father', 'she']\n",
      "['fine', 'full', 'perfect', 'that', 'i', 'taste', 'and', 'violenteth', 'in', 'a', 'sense', 'as']\n",
      "['bachelor', 'how', 'answer', 'you', 'la', 'plus', 'belle', 'katharine', 'du', 'monde', 'mon', 'tres']\n",
      "['king', 'henry', 'vi', 'ghost', 'to', 'king', 'richard', 'iii', 'when', 'i', 'was', 'mortal']\n",
      "['but', 'what', 'compact', 'mean', 'you', 'to', 'have', 'with', 'us', 'will', 'you', 'be']\n",
      "['counterfeitst', 'a', 'bark', 'a', 'sea', 'a', 'wind', 'for', 'still', 'thy', 'eyes', 'which']\n"
     ]
    }
   ],
   "source": [
    "epoch = fact.epoch(batch_size = 8, time_steps = 12)\n",
    "for x, y, progress in itertools.islice(epoch, 1):\n",
    "    print('---------------Inputs----------------')\n",
    "    for s in x:\n",
    "        print([shakespeare_words[i-2] for i in s])\n",
    "    print('---------------Targets---------------')\n",
    "    for s in y:\n",
    "        print([shakespeare_words[i-2] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train, validation and test data sets\n",
    "We have ~ 51K speehes lets put 5% to test and 5% to validation. Let's make them continues chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test / valid sample size: 5807, valid start at 38693, test start at 62593\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "test_size = int(len(shakespeare_sentences)*0.05)\n",
    "start_valid = np.random.randint(0, len(shakespeare_sentences)/2 - test_size)\n",
    "end_valid = start_valid + test_size\n",
    "start_test = np.random.randint(len(shakespeare_sentences)/2, len(shakespeare_sentences) - test_size)\n",
    "end_test = start_test + test_size\n",
    "print('Test / valid sample size: %d, valid start at %d, test start at %d'%(test_size, start_valid, start_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has 104538, validation has 5807 and test has 5807 speeches\n"
     ]
    }
   ],
   "source": [
    "s = shakespeare_sentences\n",
    "train_sentences = s[:start_valid] + s[end_valid:start_test] + s[end_test:]\n",
    "valid_sentences = s[start_valid:end_valid]\n",
    "test_sentences = s[start_test:end_test]\n",
    "print(\n",
    "    'Training has %d, validation has %d and test has %d speeches'%\n",
    "    (len(train_sentences), len(valid_sentences), len(test_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fact = EpochFactory(train_sentences)\n",
    "valid_fact = EpochFactory(valid_sentences)\n",
    "test_fact = EpochFactory(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \n",
    "    @classmethod\n",
    "    def restore(cls, session, model_directory):\n",
    "        with open(cls._parameters_file(model_directory)) as f:\n",
    "            parameters = json.load(f)\n",
    "        model = cls(\n",
    "            parameters[\"max_gradient\"],\n",
    "            parameters[\"batch_size\"], \n",
    "            parameters[\"time_steps\"], \n",
    "            parameters[\"vocabulary_size\"],\n",
    "            parameters[\"hidden_units\"], \n",
    "            parameters[\"layers\"]\n",
    "        )\n",
    "        tf.train.Saver().restore(session, cls._model_file(model_directory))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _parameters_file(model_directory):\n",
    "        return os.path.join(model_directory, \"parameters.json\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _model_file(model_directory):\n",
    "        return os.path.join(model_directory, \"model\")\n",
    "\n",
    "    def __init__(self, max_gradient, batch_size, time_steps, vocabulary_size, hidden_units, layers):\n",
    "\n",
    "        self.max_gradient = max_gradient\n",
    "        self.layers = layers\n",
    "\n",
    "        with tf.name_scope(\"Parameters\"):\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "            self.keep_probability = tf.placeholder(tf.float32, name=\"keep_probability\")\n",
    "\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input = tf.placeholder(tf.int32, shape=(batch_size, time_steps), name=\"input\")\n",
    "            self.targets = tf.placeholder(tf.int32, shape=(batch_size, time_steps), name=\"targets\")\n",
    "\n",
    "        with tf.name_scope(\"Embedding\"):\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform((vocabulary_size, hidden_units), -1.0, 1.0),\n",
    "                dtype=tf.float32,\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.input, name=\"embedded_input\")\n",
    "\n",
    "        with tf.name_scope(\"RNN\"):\n",
    "            # it is a bit harder to manage unconcatenated state\n",
    "            # for our purposes it should be OK to use concatenated state\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(hidden_units, state_is_tuple = False)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_probability)\n",
    "            rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * layers, state_is_tuple = False)\n",
    "            \n",
    "            self.reset_state = rnn_layers.zero_state(batch_size, dtype=tf.float32)\n",
    "            self.state = tf.placeholder(tf.float32, self.reset_state.get_shape(), \"state\")\n",
    "            \n",
    "            self.outputs, self.next_state = tf.nn.dynamic_rnn(\n",
    "                rnn_layers, self.embedded_input, initial_state=self.state, time_major=False)\n",
    "\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            # Concatenate all the batches into a single row.\n",
    "            self.flattened_outputs = tf.reshape(\n",
    "                tf.concat( self.outputs, 1),\n",
    "                (-1, hidden_units),\n",
    "                name=\"flattened_outputs\"\n",
    "            )\n",
    "            \n",
    "            # Project the outputs onto the vocabulary.\n",
    "            self.w = tf.get_variable(\n",
    "                \"w\", (hidden_units, vocabulary_size), initializer = tf.truncated_normal_initializer)\n",
    "            self.b = tf.get_variable(\n",
    "                \"b\", vocabulary_size, initializer = tf.truncated_normal_initializer)\n",
    "            # Compare predictions to labels.\n",
    "            self.predicted = tf.matmul(self.flattened_outputs, self.w) + self.b\n",
    "\n",
    "            # The log-perplexity for each sequence\n",
    "            self.loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.predicted],\n",
    "                [tf.concat(self.targets, -1)],\n",
    "                [tf.ones(batch_size * time_steps)]\n",
    "            )\n",
    "            # average log-perplexity over the batch\n",
    "            self.cost = tf.div(tf.reduce_sum(self.loss), batch_size, name=\"cost\")\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            self.validation_perplexity = tf.Variable(\n",
    "                dtype=tf.float32, initial_value=float(\"inf\"), trainable=False, name=\"validation_perplexity\")\n",
    "            tf.summary.scalar(self.validation_perplexity.op.name, self.validation_perplexity)\n",
    "            self.training_epoch_perplexity = tf.Variable(\n",
    "                dtype=tf.float32, initial_value=float(\"inf\"), trainable=False, name=\"training_epoch_perplexity\")\n",
    "            tf.summary.scalar(self.training_epoch_perplexity.op.name, self.training_epoch_perplexity)\n",
    "            self.iteration = tf.Variable(0, dtype=tf.int64, name=\"iteration\", trainable=False)\n",
    "            # gradient clipping\n",
    "            self.gradients, _ = tf.clip_by_global_norm(\n",
    "                tf.gradients(self.cost, tf.trainable_variables()), max_gradient, name=\"clip_gradients\")\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_step = optimizer.apply_gradients(\n",
    "                zip(self.gradients, tf.trainable_variables()), name=\"train_step\", global_step=self.iteration)\n",
    "\n",
    "        self.initialize = tf.global_variables_initializer()\n",
    "        self.summary = tf.summary.merge_all()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.input.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def time_steps(self):\n",
    "        return self.input.get_shape()[1].value\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.embedding.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def hidden_units(self):\n",
    "        return self.embedding.get_shape()[1].value\n",
    "\n",
    "    def train(\n",
    "            self, \n",
    "            session, \n",
    "            training_factory,\n",
    "            parameters,\n",
    "            exit_criteria,\n",
    "            validation,\n",
    "            logging_interval,\n",
    "            directories):\n",
    "\n",
    "        epoch = 1\n",
    "        iteration = 0\n",
    "        state = None\n",
    "        summary = self.summary_writer(directories.summary, session)\n",
    "        validation_hist = []\n",
    "        \n",
    "        try:\n",
    "            # Enumerate over the training set until exit criteria are met.\n",
    "            while True:\n",
    "                epoch_cost = 0.0\n",
    "                epoch_iteration = 0\n",
    "                \n",
    "                # rest state for each epoch\n",
    "                state = session.run(self.reset_state)\n",
    "                \n",
    "                # Enumerate over a single epoch of the training set\n",
    "                for x, y, complete in training_factory.epoch(self.batch_size, self.time_steps):\n",
    "                    _, cost, state, iteration = session.run(\n",
    "                        [self.train_step, self.cost, self.next_state, self.iteration],\n",
    "                        feed_dict={\n",
    "                            self.input: x,\n",
    "                            self.targets: y,\n",
    "                            # pass previous epoch state\n",
    "                            self.state: state,\n",
    "                            self.learning_rate: parameters.learning_rate,\n",
    "                            self.keep_probability: parameters.keep_probability\n",
    "                        })\n",
    "                    epoch_cost += cost\n",
    "                    epoch_iteration += self.time_steps\n",
    "                    if self._interval(iteration, logging_interval):\n",
    "                        tf.logging.info(\n",
    "                            \"Epoch %d (%0.4f complete), Iteration %d: epoch training perplexity %0.4f\" %\n",
    "                            (epoch, complete, iteration, self.perplexity(epoch_cost, epoch_iteration)))\n",
    "                    \n",
    "                    if validation is not None and self._interval(iteration, validation.interval):\n",
    "                        validation_perplexity = self.test(session, validation.epoch_factory)\n",
    "                        self.store_validation_perplexity(session, summary, iteration, validation_perplexity)\n",
    "                        tf.logging.info(\n",
    "                            \"Epoch %d, Iteration %d: validation perplexity %0.4f\" %\n",
    "                            (epoch, iteration, validation_perplexity))\n",
    "                        # save model if improved\n",
    "                        validation_hist.append(validation_perplexity)\n",
    "                        if (directories.model is not None) and (validation_perplexity == min(validation_hist)):\n",
    "                            model_filename = self._model_file(directories.model)\n",
    "                            tf.train.Saver().save(session, model_filename)\n",
    "                            self._write_model_parameters(directories.model)\n",
    "                            tf.logging.info(\"Saved model in %s \" % directories.model)\n",
    "                        \n",
    "                    if exit_criteria.max_iterations is not None and iteration > exit_criteria.max_iterations:\n",
    "                        raise StopTrainingException()\n",
    "\n",
    "                self.store_training_epoch_perplexity(\n",
    "                    session, summary, iteration, self.perplexity(epoch_cost, epoch_iteration))\n",
    "                epoch += 1\n",
    "                if exit_criteria.max_epochs is not None and epoch > exit_criteria.max_epochs:\n",
    "                    raise StopTrainingException()\n",
    "        except (StopTrainingException, KeyboardInterrupt):\n",
    "            pass\n",
    "        \n",
    "        tf.logging.info(\"Stop training at epoch %d, iteration %d\" % (epoch, iteration))\n",
    "        summary.close()\n",
    "\n",
    "    def _write_model_parameters(self, model_directory):\n",
    "        parameters = {\n",
    "            \"max_gradient\": self.max_gradient,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"time_steps\": self.time_steps,\n",
    "            \"vocabulary_size\": self.vocabulary_size,\n",
    "            \"hidden_units\": self.hidden_units,\n",
    "            \"layers\": self.layers\n",
    "        }\n",
    "        with open(self._parameters_file(model_directory), \"w\") as f:\n",
    "            json.dump(parameters, f, indent=4)\n",
    "\n",
    "    def test(self, session, epoch_factory):\n",
    "        state = session.run(self.reset_state)\n",
    "        epoch_cost = 0.0\n",
    "        epoch_iteration = 0\n",
    "        epoch = epoch_factory.epoch(self.batch_size, self.time_steps)\n",
    "        for x, y, _ in epoch:\n",
    "            cost, state = session.run(\n",
    "                [self.cost, self.next_state],\n",
    "                feed_dict={\n",
    "                    self.input: x, \n",
    "                    self.targets: y,\n",
    "                    self.state: state,\n",
    "                    self.keep_probability: 1.0\n",
    "                }\n",
    "            )\n",
    "            epoch_cost += cost\n",
    "            epoch_iteration += self.time_steps\n",
    "        return self.perplexity(epoch_cost, epoch_iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interval(iteration, interval):\n",
    "        return interval is not None and iteration > 1 and iteration % interval == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(cost, iterations):\n",
    "        return np.exp(cost / iterations)\n",
    "\n",
    "    def store_validation_perplexity(self, session, summary, iteration, validation_perplexity):\n",
    "        session.run(self.validation_perplexity.assign(validation_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    def store_training_epoch_perplexity(self, session, summary, iteration, training_perplexity):\n",
    "        session.run(self.training_epoch_perplexity.assign(training_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def summary_writer(summary_directory, session):\n",
    "        class NullSummaryWriter(object):\n",
    "            def add_summary(self, *args, **kwargs):\n",
    "                pass\n",
    "\n",
    "            def flush(self):\n",
    "                pass\n",
    "\n",
    "            def close(self):\n",
    "                pass\n",
    "\n",
    "        if summary_directory is not None:\n",
    "            return tf.summary.FileWriter(summary_directory, session.graph)\n",
    "        else:\n",
    "            return NullSummaryWriter()\n",
    "\n",
    "\n",
    "class StopTrainingException(Exception):\n",
    "    pass\n",
    "\n",
    "class ExitCriteria(object):\n",
    "    def __init__(self, max_iterations, max_epochs):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "class Parameters(object):\n",
    "    def __init__(self, learning_rate, keep_probability):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_probability = keep_probability\n",
    "\n",
    "class Validation(object):\n",
    "    def __init__(self, interval, epoch_factory):\n",
    "        self.interval = interval\n",
    "        self.epoch_factory = epoch_factory\n",
    "\n",
    "class Directories(object):\n",
    "    def __init__(self, model, summary):\n",
    "        self.model = model\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000231BA5C5828>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "        \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    rnn  = RNN(\n",
    "        max_gradient = 5, \n",
    "        batch_size = 64, \n",
    "        time_steps = 10, \n",
    "        # Add vocabulary slots of out of vocabulary (index 1) and padding (index 0).\n",
    "        vocabulary_size = len(shakespeare_vocabulary) + 2, \n",
    "        hidden_units = 512, \n",
    "        layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Epoch 1 (0.0459 complete), Iteration 50: epoch training perplexity 13751.1564\n",
      "INFO:tensorflow:Epoch 1 (0.0928 complete), Iteration 100: epoch training perplexity 6169.3391\n",
      "INFO:tensorflow:Epoch 1, Iteration 100: validation perplexity 1588.0681\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.1396 complete), Iteration 150: epoch training perplexity 5603.2166\n",
      "INFO:tensorflow:Epoch 1 (0.1865 complete), Iteration 200: epoch training perplexity 3810.4872\n",
      "INFO:tensorflow:Epoch 1, Iteration 200: validation perplexity 1051.8382\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.2334 complete), Iteration 250: epoch training perplexity 2913.1507\n",
      "INFO:tensorflow:Epoch 1 (0.2802 complete), Iteration 300: epoch training perplexity 2412.2825\n",
      "INFO:tensorflow:Epoch 1, Iteration 300: validation perplexity 900.0939\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.3271 complete), Iteration 350: epoch training perplexity 2089.6530\n",
      "INFO:tensorflow:Epoch 1 (0.3739 complete), Iteration 400: epoch training perplexity 1866.2931\n",
      "INFO:tensorflow:Epoch 1, Iteration 400: validation perplexity 837.8124\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.4208 complete), Iteration 450: epoch training perplexity 1697.6167\n",
      "INFO:tensorflow:Epoch 1 (0.4677 complete), Iteration 500: epoch training perplexity 1567.1506\n",
      "INFO:tensorflow:Epoch 1, Iteration 500: validation perplexity 789.6078\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.5145 complete), Iteration 550: epoch training perplexity 1471.0514\n",
      "INFO:tensorflow:Epoch 1 (0.5614 complete), Iteration 600: epoch training perplexity 1387.6404\n",
      "INFO:tensorflow:Epoch 1, Iteration 600: validation perplexity 752.6790\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.6082 complete), Iteration 650: epoch training perplexity 1313.6011\n",
      "INFO:tensorflow:Epoch 1 (0.6551 complete), Iteration 700: epoch training perplexity 1259.3634\n",
      "INFO:tensorflow:Epoch 1, Iteration 700: validation perplexity 719.0980\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.7020 complete), Iteration 750: epoch training perplexity 1210.5486\n",
      "INFO:tensorflow:Epoch 1 (0.7488 complete), Iteration 800: epoch training perplexity 1170.1801\n",
      "INFO:tensorflow:Epoch 1, Iteration 800: validation perplexity 697.3804\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.7957 complete), Iteration 850: epoch training perplexity 1129.3369\n",
      "INFO:tensorflow:Epoch 1 (0.8425 complete), Iteration 900: epoch training perplexity 1095.0866\n",
      "INFO:tensorflow:Epoch 1, Iteration 900: validation perplexity 676.7882\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.8894 complete), Iteration 950: epoch training perplexity 1061.7950\n",
      "INFO:tensorflow:Epoch 1 (0.9363 complete), Iteration 1000: epoch training perplexity 1033.5770\n",
      "INFO:tensorflow:Epoch 1, Iteration 1000: validation perplexity 656.3663\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.9831 complete), Iteration 1050: epoch training perplexity 1011.3763\n",
      "INFO:tensorflow:Epoch 2 (0.0300 complete), Iteration 1100: epoch training perplexity 549.9885\n",
      "INFO:tensorflow:Epoch 2, Iteration 1100: validation perplexity 642.6925\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.0769 complete), Iteration 1150: epoch training perplexity 535.4962\n",
      "INFO:tensorflow:Epoch 2 (0.1237 complete), Iteration 1200: epoch training perplexity 539.7211\n",
      "INFO:tensorflow:Epoch 2, Iteration 1200: validation perplexity 629.7650\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.1706 complete), Iteration 1250: epoch training perplexity 526.8608\n",
      "INFO:tensorflow:Epoch 2 (0.2174 complete), Iteration 1300: epoch training perplexity 516.5688\n",
      "INFO:tensorflow:Epoch 2, Iteration 1300: validation perplexity 617.7294\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.2643 complete), Iteration 1350: epoch training perplexity 510.7423\n",
      "INFO:tensorflow:Epoch 2 (0.3112 complete), Iteration 1400: epoch training perplexity 507.4601\n",
      "INFO:tensorflow:Epoch 2, Iteration 1400: validation perplexity 614.5520\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.3580 complete), Iteration 1450: epoch training perplexity 502.7263\n",
      "INFO:tensorflow:Epoch 2 (0.4049 complete), Iteration 1500: epoch training perplexity 501.7825\n",
      "INFO:tensorflow:Epoch 2, Iteration 1500: validation perplexity 604.6883\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.4517 complete), Iteration 1550: epoch training perplexity 498.5387\n",
      "INFO:tensorflow:Epoch 2 (0.4986 complete), Iteration 1600: epoch training perplexity 496.8019\n",
      "INFO:tensorflow:Epoch 2, Iteration 1600: validation perplexity 597.3765\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.5455 complete), Iteration 1650: epoch training perplexity 496.8387\n",
      "INFO:tensorflow:Epoch 2 (0.5923 complete), Iteration 1700: epoch training perplexity 493.9633\n",
      "INFO:tensorflow:Epoch 2, Iteration 1700: validation perplexity 589.3824\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.6392 complete), Iteration 1750: epoch training perplexity 492.6059\n",
      "INFO:tensorflow:Epoch 2 (0.6860 complete), Iteration 1800: epoch training perplexity 492.7848\n",
      "INFO:tensorflow:Epoch 2, Iteration 1800: validation perplexity 584.5163\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.7329 complete), Iteration 1850: epoch training perplexity 493.3920\n",
      "INFO:tensorflow:Epoch 2 (0.7798 complete), Iteration 1900: epoch training perplexity 491.6866\n",
      "INFO:tensorflow:Epoch 2, Iteration 1900: validation perplexity 579.1880\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.8266 complete), Iteration 1950: epoch training perplexity 490.0674\n",
      "INFO:tensorflow:Epoch 2 (0.8735 complete), Iteration 2000: epoch training perplexity 487.7310\n",
      "INFO:tensorflow:Epoch 2, Iteration 2000: validation perplexity 574.7829\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.9203 complete), Iteration 2050: epoch training perplexity 486.7548\n",
      "INFO:tensorflow:Epoch 2 (0.9672 complete), Iteration 2100: epoch training perplexity 486.0742\n",
      "INFO:tensorflow:Epoch 2, Iteration 2100: validation perplexity 567.1102\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.0141 complete), Iteration 2150: epoch training perplexity 437.5542\n",
      "INFO:tensorflow:Epoch 3 (0.0609 complete), Iteration 2200: epoch training perplexity 424.7309\n",
      "INFO:tensorflow:Epoch 3, Iteration 2200: validation perplexity 566.4122\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.1078 complete), Iteration 2250: epoch training perplexity 418.2388\n",
      "INFO:tensorflow:Epoch 3 (0.1546 complete), Iteration 2300: epoch training perplexity 411.5199\n",
      "INFO:tensorflow:Epoch 3, Iteration 2300: validation perplexity 562.9930\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.2015 complete), Iteration 2350: epoch training perplexity 404.9624\n",
      "INFO:tensorflow:Epoch 3 (0.2484 complete), Iteration 2400: epoch training perplexity 399.2577\n",
      "INFO:tensorflow:Epoch 3, Iteration 2400: validation perplexity 558.6998\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.2952 complete), Iteration 2450: epoch training perplexity 397.6178\n",
      "INFO:tensorflow:Epoch 3 (0.3421 complete), Iteration 2500: epoch training perplexity 394.0932\n",
      "INFO:tensorflow:Epoch 3, Iteration 2500: validation perplexity 561.6738\n",
      "INFO:tensorflow:Epoch 3 (0.3889 complete), Iteration 2550: epoch training perplexity 392.6849\n",
      "INFO:tensorflow:Epoch 3 (0.4358 complete), Iteration 2600: epoch training perplexity 390.4742\n",
      "INFO:tensorflow:Epoch 3, Iteration 2600: validation perplexity 555.6240\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.4827 complete), Iteration 2650: epoch training perplexity 389.4546\n",
      "INFO:tensorflow:Epoch 3 (0.5295 complete), Iteration 2700: epoch training perplexity 389.9143\n",
      "INFO:tensorflow:Epoch 3, Iteration 2700: validation perplexity 553.8777\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.5764 complete), Iteration 2750: epoch training perplexity 389.8374\n",
      "INFO:tensorflow:Epoch 3 (0.6232 complete), Iteration 2800: epoch training perplexity 388.0189\n",
      "INFO:tensorflow:Epoch 3, Iteration 2800: validation perplexity 552.4581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.6701 complete), Iteration 2850: epoch training perplexity 388.9852\n",
      "INFO:tensorflow:Epoch 3 (0.7170 complete), Iteration 2900: epoch training perplexity 389.8953\n",
      "INFO:tensorflow:Epoch 3, Iteration 2900: validation perplexity 544.8230\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.7638 complete), Iteration 2950: epoch training perplexity 390.0539\n",
      "INFO:tensorflow:Epoch 3 (0.8107 complete), Iteration 3000: epoch training perplexity 389.6283\n",
      "INFO:tensorflow:Epoch 3, Iteration 3000: validation perplexity 545.5295\n",
      "INFO:tensorflow:Epoch 3 (0.8575 complete), Iteration 3050: epoch training perplexity 388.7897\n",
      "INFO:tensorflow:Epoch 3 (0.9044 complete), Iteration 3100: epoch training perplexity 387.9840\n",
      "INFO:tensorflow:Epoch 3, Iteration 3100: validation perplexity 543.3475\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.9513 complete), Iteration 3150: epoch training perplexity 387.7841\n",
      "INFO:tensorflow:Epoch 3 (0.9981 complete), Iteration 3200: epoch training perplexity 388.0736\n",
      "INFO:tensorflow:Epoch 3, Iteration 3200: validation perplexity 540.3628\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 4 (0.0450 complete), Iteration 3250: epoch training perplexity 356.2568\n",
      "INFO:tensorflow:Epoch 4 (0.0918 complete), Iteration 3300: epoch training perplexity 343.2213\n",
      "INFO:tensorflow:Epoch 4, Iteration 3300: validation perplexity 542.1007\n",
      "INFO:tensorflow:Epoch 4 (0.1387 complete), Iteration 3350: epoch training perplexity 345.9138\n",
      "INFO:tensorflow:Epoch 4 (0.1856 complete), Iteration 3400: epoch training perplexity 341.1120\n",
      "INFO:tensorflow:Epoch 4, Iteration 3400: validation perplexity 539.6684\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 4 (0.2324 complete), Iteration 3450: epoch training perplexity 335.1946\n",
      "INFO:tensorflow:Epoch 4 (0.2793 complete), Iteration 3500: epoch training perplexity 332.5722\n",
      "INFO:tensorflow:Epoch 4, Iteration 3500: validation perplexity 543.6229\n",
      "INFO:tensorflow:Epoch 4 (0.3261 complete), Iteration 3550: epoch training perplexity 330.3846\n",
      "INFO:tensorflow:Epoch 4 (0.3730 complete), Iteration 3600: epoch training perplexity 328.0071\n",
      "INFO:tensorflow:Epoch 4, Iteration 3600: validation perplexity 542.4196\n",
      "INFO:tensorflow:Epoch 4 (0.4199 complete), Iteration 3650: epoch training perplexity 326.8045\n",
      "INFO:tensorflow:Epoch 4 (0.4667 complete), Iteration 3700: epoch training perplexity 325.5731\n",
      "INFO:tensorflow:Epoch 4, Iteration 3700: validation perplexity 544.4541\n",
      "INFO:tensorflow:Epoch 4 (0.5136 complete), Iteration 3750: epoch training perplexity 326.1933\n",
      "INFO:tensorflow:Epoch 4 (0.5604 complete), Iteration 3800: epoch training perplexity 326.0336\n",
      "INFO:tensorflow:Epoch 4, Iteration 3800: validation perplexity 543.1280\n",
      "INFO:tensorflow:Epoch 4 (0.6073 complete), Iteration 3850: epoch training perplexity 324.4163\n",
      "INFO:tensorflow:Epoch 4 (0.6542 complete), Iteration 3900: epoch training perplexity 325.3471\n",
      "INFO:tensorflow:Epoch 4, Iteration 3900: validation perplexity 538.9638\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 4 (0.7010 complete), Iteration 3950: epoch training perplexity 325.6067\n",
      "INFO:tensorflow:Epoch 4 (0.7479 complete), Iteration 4000: epoch training perplexity 326.7541\n",
      "INFO:tensorflow:Epoch 4, Iteration 4000: validation perplexity 542.4216\n",
      "INFO:tensorflow:Epoch 4 (0.7948 complete), Iteration 4050: epoch training perplexity 326.4620\n",
      "INFO:tensorflow:Epoch 4 (0.8416 complete), Iteration 4100: epoch training perplexity 326.1985\n",
      "INFO:tensorflow:Epoch 4, Iteration 4100: validation perplexity 540.3664\n",
      "INFO:tensorflow:Epoch 4 (0.8885 complete), Iteration 4150: epoch training perplexity 325.4205\n",
      "INFO:tensorflow:Epoch 4 (0.9353 complete), Iteration 4200: epoch training perplexity 325.2618\n",
      "INFO:tensorflow:Epoch 4, Iteration 4200: validation perplexity 541.5568\n",
      "INFO:tensorflow:Epoch 4 (0.9822 complete), Iteration 4250: epoch training perplexity 325.4785\n",
      "INFO:tensorflow:Epoch 5 (0.0291 complete), Iteration 4300: epoch training perplexity 305.7196\n",
      "INFO:tensorflow:Epoch 5, Iteration 4300: validation perplexity 541.2196\n",
      "INFO:tensorflow:Epoch 5 (0.0759 complete), Iteration 4350: epoch training perplexity 297.0375\n",
      "INFO:tensorflow:Epoch 5 (0.1228 complete), Iteration 4400: epoch training perplexity 297.1730\n",
      "INFO:tensorflow:Epoch 5, Iteration 4400: validation perplexity 543.8996\n",
      "INFO:tensorflow:Epoch 5 (0.1696 complete), Iteration 4450: epoch training perplexity 292.4609\n",
      "INFO:tensorflow:Epoch 5 (0.2165 complete), Iteration 4500: epoch training perplexity 287.7834\n",
      "INFO:tensorflow:Epoch 5, Iteration 4500: validation perplexity 544.7155\n",
      "INFO:tensorflow:Epoch 5 (0.2634 complete), Iteration 4550: epoch training perplexity 284.3272\n",
      "INFO:tensorflow:Epoch 5 (0.3102 complete), Iteration 4600: epoch training perplexity 283.0634\n",
      "INFO:tensorflow:Epoch 5, Iteration 4600: validation perplexity 552.4119\n",
      "INFO:tensorflow:Epoch 5 (0.3571 complete), Iteration 4650: epoch training perplexity 280.5044\n",
      "INFO:tensorflow:Epoch 5 (0.4039 complete), Iteration 4700: epoch training perplexity 280.1091\n",
      "INFO:tensorflow:Epoch 5, Iteration 4700: validation perplexity 549.8349\n",
      "INFO:tensorflow:Epoch 5 (0.4508 complete), Iteration 4750: epoch training perplexity 279.2265\n",
      "INFO:tensorflow:Epoch 5 (0.4977 complete), Iteration 4800: epoch training perplexity 278.3687\n",
      "INFO:tensorflow:Epoch 5, Iteration 4800: validation perplexity 551.3597\n",
      "INFO:tensorflow:Epoch 5 (0.5445 complete), Iteration 4850: epoch training perplexity 278.9506\n",
      "INFO:tensorflow:Epoch 5 (0.5914 complete), Iteration 4900: epoch training perplexity 278.1305\n",
      "INFO:tensorflow:Epoch 5, Iteration 4900: validation perplexity 553.6042\n",
      "INFO:tensorflow:Epoch 5 (0.6382 complete), Iteration 4950: epoch training perplexity 277.8209\n",
      "INFO:tensorflow:Epoch 5 (0.6851 complete), Iteration 5000: epoch training perplexity 278.5747\n",
      "INFO:tensorflow:Epoch 5, Iteration 5000: validation perplexity 551.4566\n",
      "INFO:tensorflow:Epoch 5 (0.7320 complete), Iteration 5050: epoch training perplexity 279.3975\n",
      "INFO:tensorflow:Epoch 5 (0.7788 complete), Iteration 5100: epoch training perplexity 279.5414\n",
      "INFO:tensorflow:Epoch 5, Iteration 5100: validation perplexity 545.2983\n",
      "INFO:tensorflow:Epoch 5 (0.8257 complete), Iteration 5150: epoch training perplexity 279.2363\n",
      "INFO:tensorflow:Epoch 5 (0.8725 complete), Iteration 5200: epoch training perplexity 278.5350\n",
      "INFO:tensorflow:Epoch 5, Iteration 5200: validation perplexity 549.8142\n",
      "INFO:tensorflow:Epoch 5 (0.9194 complete), Iteration 5250: epoch training perplexity 278.6773\n",
      "INFO:tensorflow:Epoch 5 (0.9663 complete), Iteration 5300: epoch training perplexity 278.4654\n",
      "INFO:tensorflow:Epoch 5, Iteration 5300: validation perplexity 549.5733\n",
      "INFO:tensorflow:Stop training at epoch 6, iteration 5335\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree('model', ignore_errors = True)\n",
    "os.makedirs('model')\n",
    "shutil.rmtree('summary', ignore_errors = True)\n",
    "os.makedirs('summary')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    rnn.train(\n",
    "        session = sess, \n",
    "        training_factory = train_fact, \n",
    "        parameters = Parameters(learning_rate = 0.001, keep_probability = 0.7),\n",
    "        exit_criteria = ExitCriteria(max_iterations = None, max_epochs = 5),\n",
    "        validation = Validation(interval = 100, epoch_factory = valid_fact), \n",
    "        logging_interval = 50, \n",
    "        directories = Directories('model', 'summary')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate RNN model performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000231EE170048>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Restoring parameters from model\\model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    rnn = RNN.restore(sess, 'model')\n",
    "    rnn_test_perplexity = rnn.test(sess, test_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our RNN language model we build baseline unigram model to compare the perplexity on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniGram(object):\n",
    "\n",
    "    def __init__(self, texts):\n",
    "        self.counter = collections.Counter(WordGenerator(texts))\n",
    "        self.corpus_size = sum(self.counter.values())\n",
    "\n",
    "    # word probability is the word freq divided by the corpus size\n",
    "    def word_proba(self, word):\n",
    "        freq = self.counter[word]\n",
    "        return float(freq)/float(self.corpus_size)\n",
    "    \n",
    "    # assumes independence\n",
    "    def sentence_proba(self, sentence):\n",
    "        word_probs = [self.word_proba(w) for w in sentence]\n",
    "        return reduce(lambda x, y: x*y, word_probs)\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        p = self.sentence_proba(sentence)\n",
    "        return pow(p, -1.0/float(len(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniGram = UniGram(shakespeare_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'in' is 0.013105\n",
      "Probability of 'delivering' is 0.000005\n",
      "Probability of 'my' is 0.014527\n",
      "Probability of 'son' is 0.000650\n",
      "Probability of 'from' is 0.003156\n",
      "Probability of 'me' is 0.009196\n",
      "Sentence probability is  1.8897761650936493e-17\n",
      "Sentence perplexity is  612.7240664727524\n"
     ]
    }
   ],
   "source": [
    "s = ['in', 'delivering', 'my', 'son', 'from', 'me']\n",
    "for w in s:\n",
    "    print(\"Probability of '%s' is %f\" % (w, uniGram.word_proba(w)))\n",
    "print('Sentence probability is ' , uniGram.sentence_proba(s))\n",
    "print('Sentence perplexity is ' , uniGram.perplexity(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate perplexity for each sentence and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = test_fact.epoch(batch_size = 1, time_steps = 10)\n",
    "pp = []\n",
    "for x, y, progress in test_epoch:\n",
    "    for s in x:\n",
    "        pp.append(uniGram.perplexity([shakespeare_words[i-2] for i in s]))\n",
    "uni_test_perplexity = np.mean(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model test perplexity  477.119320215\n",
      "Unigram model test perplexity  1345.33048967\n"
     ]
    }
   ],
   "source": [
    "print('RNN model test perplexity ', rnn_test_perplexity)\n",
    "print('Unigram model test perplexity ', uni_test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our RNN language model shows superior performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
