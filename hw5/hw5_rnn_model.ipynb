{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN based word-level language model for the Shakespeare plays and poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTLK version 3.2.3\n",
      "TensorFlow version 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "print('NTLK version', nltk.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adubitskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_plays.pickle', 'rb') as handle:\n",
    "    speeches = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_poetries.pickle', 'rb') as handle:\n",
    "    poetries = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare_texts = [s['speech_text'] for s in speeches] + [p['text'] for p in poetries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sample (first ten speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In delivering my son from me, I bury a second husband.', \"And I in going, madam, weep o'er my father's death\\nanew: but I must attend his majesty's command, to\\nwhom I am now in ward, evermore in subjection.\", 'You shall find of the king a husband, madam; you,\\nsir, a father: he that so generally is at all times\\ngood must of necessity hold his virtue to you; whose\\nworthiness would stir it up where it wanted rather\\nthan lack it where there is such abundance.', \"What hope is there of his majesty's amendment?\", 'He hath abandoned his physicians, madam; under whose\\npractises he hath persecuted time with hope, and\\nfinds no other advantage in the process but only the\\nlosing of hope by time.', \"This young gentlewoman had a father,--O, that\\n'had'! how sad a passage 'tis!--whose skill was\\nalmost as great as his honesty; had it stretched so\\nfar, would have made nature immortal, and death\\nshould have play for lack of work. Would, for the\\nking's sake, he were living! I think it would be\\nthe death of the king's disease.\", 'How called you the man you speak of, madam?', 'He was famous, sir, in his profession, and it was\\nhis great right to be so: Gerard de Narbon.', 'He was excellent indeed, madam: the king very\\nlately spoke of him admiringly and mourningly: he\\nwas skilful enough to have lived still, if knowledge\\ncould be set up against mortality.', 'What is it, my good lord, the king languishes of?']\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_texts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've decided to split sentences on new lines as well as usual punctuation. It seems liike a reasonabe decision to consider every line as a sentence in the case of poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text in self.texts:\n",
    "            for s in self.tokenizer.tokenize(text.lower()):\n",
    "                for p in s.splitlines():\n",
    "                    yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in delivering my son from me, i bury a second husband.\n",
      "----------------------------------------\n",
      "and i in going, madam, weep o'er my father's death\n",
      "----------------------------------------\n",
      "anew: but i must attend his majesty's command, to\n",
      "----------------------------------------\n",
      "whom i am now in ward, evermore in subjection.\n",
      "----------------------------------------\n",
      "you shall find of the king a husband, madam; you,\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(SentenceGenerator(shakespeare_texts), 5):\n",
    "    print(s)\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordGenerator(object):\n",
    "    \n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.trans = str.maketrans('','', string.punctuation)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for s in SentenceGenerator(self.texts):\n",
    "            for w in s.translate(self.trans).split():\n",
    "                yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "delivering\n",
      "my\n",
      "son\n",
      "from\n",
      "me\n",
      "i\n",
      "bury\n",
      "a\n",
      "second\n",
      "husband\n",
      "and\n",
      "i\n",
      "in\n",
      "going\n",
      "madam\n",
      "weep\n",
      "oer\n",
      "my\n",
      "fathers\n"
     ]
    }
   ],
   "source": [
    "for w in itertools.islice(WordGenerator(shakespeare_texts), 20):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary allows as to encode words into integers and feed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(texts):\n",
    "    wordGen = WordGenerator(texts)\n",
    "    counter = collections.Counter(wordGen)\n",
    "    # unique list of words with the frequencies\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # unique list of words\n",
    "    words, x = list(zip(*count_pairs))\n",
    "    # reserve 0 for padding, 1 for out of vocabulary\n",
    "    start_index = 2\n",
    "    return words, dict(zip(words, range(start_index, len(words) + start_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare vocabulary size: 27042\n"
     ]
    }
   ],
   "source": [
    "shakespeare_words, shakespeare_vocabulary = build_vocabulary(shakespeare_texts)\n",
    "print('Shakespeare vocabulary size:', len(shakespeare_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we could translate back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7114 transport\n"
     ]
    }
   ],
   "source": [
    "i = shakespeare_vocabulary['transport']\n",
    "print(i, shakespeare_words[i- 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates sentences using the words Ids\n",
    "class EmbeddedSentenceGenerator(object):\n",
    "\n",
    "    def __init__(self, texts, vocabulary):\n",
    "        self.texts = texts\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        trans = str.maketrans('','', string.punctuation)\n",
    "\n",
    "        for s in SentenceGenerator(self.texts):\n",
    "            yield [ self.vocabulary[w] if w in self.vocabulary else 1 for w in s.translate(trans).split()]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8418, 9, 186, 48, 15, 4, 1908, 7, 716, 296]\n",
      "----------------------------------------\n",
      "[3, 4, 11, 793, 190, 524, 420, 9, 366, 128]\n",
      "----------------------------------------\n",
      "[5067, 23, 4, 88, 675, 18, 4610, 561, 5]\n",
      "----------------------------------------\n",
      "[226, 4, 64, 45, 11, 3994, 2499, 11, 6380]\n",
      "----------------------------------------\n",
      "[8, 39, 188, 6, 2, 107, 7, 296, 190, 8]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary), 5):\n",
    "    print(s)\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we could recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'delivering', 'my', 'son', 'from', 'me', 'i', 'bury', 'a', 'second', 'husband']\n",
      "----------------------------------------\n",
      "['and', 'i', 'in', 'going', 'madam', 'weep', 'oer', 'my', 'fathers', 'death']\n",
      "----------------------------------------\n",
      "['anew', 'but', 'i', 'must', 'attend', 'his', 'majestys', 'command', 'to']\n",
      "----------------------------------------\n",
      "['whom', 'i', 'am', 'now', 'in', 'ward', 'evermore', 'in', 'subjection']\n",
      "----------------------------------------\n",
      "['you', 'shall', 'find', 'of', 'the', 'king', 'a', 'husband', 'madam', 'you']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary), 5):\n",
    "    print([shakespeare_words[i-2] for i in s])\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide how to deal with sentences of different lengths. It can get pretty complex with padding for the shorter sentences and catrrying over the longer sentences to the next batch. For the purpose of this engagement we've decided to go with a simpler araangement:  \n",
    "\n",
    "1. Split entire sample into tokens, so we have a one-dimentional array of N tokens.\n",
    "2. Split the array into 'batch size' of one dimensional arrays (batches).\n",
    "3. Split every batch into chuncks of 'time steps' each. \n",
    "\n",
    "We going to feed our network with input tensors of shape (batch size, sequence size). The output is just a shifted version of the input (one word forward).   \n",
    "If this sounds dodgy and suspicious please see the examples below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# factory for creating the epochs\n",
    "class EpochFactory(object):\n",
    "    def __init__(self, sentences):\n",
    "        # list of tokens\n",
    "        self.raw_data = [word for sentence in sentences for word in sentence]\n",
    "\n",
    "    def epoch(self, batch_size, time_steps):\n",
    "        return Epoch(self.raw_data, batch_size, time_steps)\n",
    "    \n",
    "# provides one epoch worth of data\n",
    "# time steps is the sequence length\n",
    "class Epoch(object):\n",
    "    def __init__(self, words, batch_size, time_steps):\n",
    "        self.raw_data = words\n",
    "        self.batch_size = batch_size\n",
    "        self.time_steps = time_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "\n",
    "        data_len = np.size(self.raw_data)\n",
    "        # number of batches\n",
    "        batch_len = data_len // self.batch_size\n",
    "        # number of chunks\n",
    "        chunk_len = (batch_len - 1) // self.time_steps\n",
    "\n",
    "        assert (chunk_len > 0), \"chunk_len == 0, decrease batch_size or num_steps\"\n",
    "\n",
    "        # reshape to have batches\n",
    "        # we loose some data at the end (batch size minus one maximum number of tokens)\n",
    "        data = np.reshape(self.raw_data[0 : self.batch_size * batch_len], [self.batch_size, batch_len])\n",
    "        \n",
    "        # draw samples\n",
    "        # each sample is an array of size (batch size, time step)\n",
    "        for i in range(chunk_len):\n",
    "            x = data[0 : self.batch_size , i * self.time_steps     : (i + 1) * self.time_steps] \n",
    "            y = data[0 : self.batch_size , i * self.time_steps + 1 : (i + 1) * self.time_steps + 1]\n",
    "            yield x, y, float(i)/float(chunk_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 116152 sentences\n"
     ]
    }
   ],
   "source": [
    "shakespeare_sentences = [s for s in EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary)]\n",
    "print('Total %d sentences'%len(shakespeare_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do want to follow the language structure so in our circumstances the best way would be to use the average sentences length as the sequence length. I think I will make it 8 since I like numbers to be 2 power something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median sentence lengh  7.0\n",
      "Mean sentence lengh  6.54403712377\n"
     ]
    }
   ],
   "source": [
    "s_lengths = [len(s) for s in shakespeare_sentences]\n",
    "print('Median sentence lengh ', np.median(s_lengths))\n",
    "print('Mean sentence lengh ', np.mean(s_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fact = EpochFactory(shakespeare_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Inputs----------------\n",
      "[11, 8418, 9, 186, 48, 15, 4, 1908, 7, 716, 296, 3]\n",
      "[938, 5, 21340, 53, 37, 14, 83, 19, 1280, 47, 2, 1456]\n",
      "[8, 38, 75, 78, 731, 140, 37, 615, 2148, 19, 31, 140]\n",
      "[12, 832, 237, 1294, 10, 4, 905, 3, 26418, 11, 7, 627]\n",
      "[7, 3473, 63, 262, 8, 1469, 13753, 15853, 2357, 9970, 13396, 7688]\n",
      "[6, 107, 1180, 14837, 1438, 5, 107, 664, 3119, 65, 4, 59]\n",
      "[28, 23, 30, 3655, 334, 8, 5, 24, 16, 86, 29, 8]\n",
      "[26, 12081, 7, 1424, 7, 393, 7, 509, 17, 173, 32, 134]\n",
      "---------------Targets---------------\n",
      "[8418, 9, 186, 48, 15, 4, 1908, 7, 716, 296, 3, 4]\n",
      "[5, 21340, 53, 37, 14, 83, 19, 1280, 47, 2, 1456, 124]\n",
      "[38, 75, 78, 731, 140, 37, 615, 2148, 19, 31, 140, 46]\n",
      "[832, 237, 1294, 10, 4, 905, 3, 26418, 11, 7, 627, 25]\n",
      "[3473, 63, 262, 8, 1469, 13753, 15853, 2357, 9970, 13396, 7688, 14624]\n",
      "[107, 1180, 14837, 1438, 5, 107, 664, 3119, 65, 4, 59, 756]\n",
      "[23, 30, 3655, 334, 8, 5, 24, 16, 86, 29, 8, 19]\n",
      "[12081, 7, 1424, 7, 393, 7, 509, 17, 173, 32, 134, 50]\n"
     ]
    }
   ],
   "source": [
    "epoch = fact.epoch(batch_size = 8, time_steps = 12)\n",
    "for x, y, progress in itertools.islice(epoch, 1):\n",
    "    print('---------------Inputs----------------')\n",
    "    for s in x:\n",
    "        print([i for i in s])\n",
    "    print('---------------Targets---------------')\n",
    "    for s in y:\n",
    "        print([i for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample chunk recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Inputs----------------\n",
      "['in', 'delivering', 'my', 'son', 'from', 'me', 'i', 'bury', 'a', 'second', 'husband', 'and']\n",
      "['int', 'to', 'mete', 'at', 'if', 'it', 'may', 'be', 'wide', 'o', 'the', 'bow']\n",
      "['you', 'are', 'like', 'an', 'honourable', 'father', 'if', 'signior', 'leonato', 'be', 'her', 'father']\n",
      "['is', 'fine', 'full', 'perfect', 'that', 'i', 'taste', 'and', 'violenteth', 'in', 'a', 'sense']\n",
      "['a', 'bachelor', 'how', 'answer', 'you', 'la', 'plus', 'belle', 'katharine', 'du', 'monde', 'mon']\n",
      "['of', 'king', 'henry', 'vi', 'ghost', 'to', 'king', 'richard', 'iii', 'when', 'i', 'was']\n",
      "['so', 'but', 'what', 'compact', 'mean', 'you', 'to', 'have', 'with', 'us', 'will', 'you']\n",
      "['thou', 'counterfeitst', 'a', 'bark', 'a', 'sea', 'a', 'wind', 'for', 'still', 'thy', 'eyes']\n",
      "---------------Targets---------------\n",
      "['delivering', 'my', 'son', 'from', 'me', 'i', 'bury', 'a', 'second', 'husband', 'and', 'i']\n",
      "['to', 'mete', 'at', 'if', 'it', 'may', 'be', 'wide', 'o', 'the', 'bow', 'hand']\n",
      "['are', 'like', 'an', 'honourable', 'father', 'if', 'signior', 'leonato', 'be', 'her', 'father', 'she']\n",
      "['fine', 'full', 'perfect', 'that', 'i', 'taste', 'and', 'violenteth', 'in', 'a', 'sense', 'as']\n",
      "['bachelor', 'how', 'answer', 'you', 'la', 'plus', 'belle', 'katharine', 'du', 'monde', 'mon', 'tres']\n",
      "['king', 'henry', 'vi', 'ghost', 'to', 'king', 'richard', 'iii', 'when', 'i', 'was', 'mortal']\n",
      "['but', 'what', 'compact', 'mean', 'you', 'to', 'have', 'with', 'us', 'will', 'you', 'be']\n",
      "['counterfeitst', 'a', 'bark', 'a', 'sea', 'a', 'wind', 'for', 'still', 'thy', 'eyes', 'which']\n"
     ]
    }
   ],
   "source": [
    "epoch = fact.epoch(batch_size = 8, time_steps = 12)\n",
    "for x, y, progress in itertools.islice(epoch, 1):\n",
    "    print('---------------Inputs----------------')\n",
    "    for s in x:\n",
    "        print([shakespeare_words[i-2] for i in s])\n",
    "    print('---------------Targets---------------')\n",
    "    for s in y:\n",
    "        print([shakespeare_words[i-2] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train, validation and test data sets\n",
    "We have ~ 51K speeches lets put 5% to test and 5% to validation. Let's make them continues chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test / valid sample size: 5807, valid start at 38693, test start at 62593\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "test_size = int(len(shakespeare_sentences)*0.05)\n",
    "\n",
    "start_valid = np.random.randint(0, len(shakespeare_sentences)/2 - test_size)\n",
    "end_valid = start_valid + test_size\n",
    "\n",
    "start_test = np.random.randint(len(shakespeare_sentences)/2, len(shakespeare_sentences) - test_size)\n",
    "end_test = start_test + test_size\n",
    "\n",
    "print('Test / valid sample size: %d, valid start at %d, test start at %d'%(test_size, start_valid, start_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has 104538, validation has 5807 and test has 5807 sentences\n"
     ]
    }
   ],
   "source": [
    "s = shakespeare_sentences\n",
    "train_sentences = s[:start_valid] + s[end_valid:start_test] + s[end_test:]\n",
    "valid_sentences = s[start_valid:end_valid]\n",
    "test_sentences = s[start_test:end_test]\n",
    "print(\n",
    "    'Training has %d, validation has %d and test has %d sentences'%\n",
    "    (len(train_sentences), len(valid_sentences), len(test_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fact = EpochFactory(train_sentences)\n",
    "valid_fact = EpochFactory(valid_sentences)\n",
    "test_fact = EpochFactory(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \n",
    "    @classmethod\n",
    "    def restore(cls, session, model_directory):\n",
    "        with open(cls._parameters_file(model_directory)) as f:\n",
    "            parameters = json.load(f)\n",
    "        model = cls(\n",
    "            parameters[\"max_gradient\"],\n",
    "            parameters[\"batch_size\"], \n",
    "            parameters[\"time_steps\"], \n",
    "            parameters[\"vocabulary_size\"],\n",
    "            parameters[\"hidden_units\"], \n",
    "            parameters[\"layers\"]\n",
    "        )\n",
    "        tf.train.Saver().restore(session, cls._model_file(model_directory))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _parameters_file(model_directory):\n",
    "        return os.path.join(model_directory, \"parameters.json\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _model_file(model_directory):\n",
    "        return os.path.join(model_directory, \"model\")\n",
    "\n",
    "    def __init__(self, max_gradient, batch_size, time_steps, vocabulary_size, hidden_units, layers):\n",
    "\n",
    "        self.max_gradient = max_gradient\n",
    "        self.layers = layers\n",
    "\n",
    "        with tf.name_scope(\"Parameters\"):\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "            self.keep_probability = tf.placeholder(tf.float32, name=\"keep_probability\")\n",
    "\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input = tf.placeholder(tf.int32, shape=(batch_size, time_steps), name=\"input\")\n",
    "            self.targets = tf.placeholder(tf.int32, shape=(batch_size, time_steps), name=\"targets\")\n",
    "\n",
    "        with tf.name_scope(\"Embedding\"):\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform((vocabulary_size, hidden_units), -1.0, 1.0),\n",
    "                dtype=tf.float32,\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.input, name=\"embedded_input\")\n",
    "\n",
    "        with tf.name_scope(\"RNN\"):\n",
    "            # it is a bit harder to manage unconcatenated state\n",
    "            # for our purposes it should be OK to use concatenated state\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(hidden_units, state_is_tuple = False)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_probability)\n",
    "            rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * layers, state_is_tuple = False)\n",
    "            \n",
    "            self.reset_state = rnn_layers.zero_state(batch_size, dtype=tf.float32)\n",
    "            self.state = tf.placeholder(tf.float32, self.reset_state.get_shape(), \"state\")\n",
    "            \n",
    "            self.outputs, self.next_state = tf.nn.dynamic_rnn(\n",
    "                rnn_layers, self.embedded_input, initial_state=self.state, time_major=False)\n",
    "\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            # Concatenate all the batches into a single row.\n",
    "            self.flattened_outputs = tf.reshape(\n",
    "                tf.concat( self.outputs, 1),\n",
    "                (-1, hidden_units),\n",
    "                name=\"flattened_outputs\"\n",
    "            )\n",
    "            \n",
    "            # Project the outputs onto the vocabulary.\n",
    "            self.w = tf.get_variable(\n",
    "                \"w\", (hidden_units, vocabulary_size), initializer = tf.truncated_normal_initializer)\n",
    "            self.b = tf.get_variable(\n",
    "                \"b\", vocabulary_size, initializer = tf.truncated_normal_initializer)\n",
    "            # Compare predictions to labels.\n",
    "            self.predicted = tf.matmul(self.flattened_outputs, self.w) + self.b\n",
    "\n",
    "            # The log-perplexity for each sequence\n",
    "            self.loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.predicted],\n",
    "                [tf.concat(self.targets, -1)],\n",
    "                [tf.ones(batch_size * time_steps)]\n",
    "            )\n",
    "            # average log-perplexity over the batch\n",
    "            self.cost = tf.div(tf.reduce_sum(self.loss), batch_size, name=\"cost\")\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            self.validation_perplexity = tf.Variable(\n",
    "                dtype=tf.float32, initial_value=float(\"inf\"), trainable=False, name=\"validation_perplexity\")\n",
    "            tf.summary.scalar(self.validation_perplexity.op.name, self.validation_perplexity)\n",
    "            self.training_epoch_perplexity = tf.Variable(\n",
    "                dtype=tf.float32, initial_value=float(\"inf\"), trainable=False, name=\"training_epoch_perplexity\")\n",
    "            tf.summary.scalar(self.training_epoch_perplexity.op.name, self.training_epoch_perplexity)\n",
    "            self.iteration = tf.Variable(0, dtype=tf.int64, name=\"iteration\", trainable=False)\n",
    "            # gradient clipping\n",
    "            self.gradients, _ = tf.clip_by_global_norm(\n",
    "                tf.gradients(self.cost, tf.trainable_variables()), max_gradient, name=\"clip_gradients\")\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_step = optimizer.apply_gradients(\n",
    "                zip(self.gradients, tf.trainable_variables()), name=\"train_step\", global_step=self.iteration)\n",
    "\n",
    "        self.initialize = tf.global_variables_initializer()\n",
    "        self.summary = tf.summary.merge_all()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.input.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def time_steps(self):\n",
    "        return self.input.get_shape()[1].value\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.embedding.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def hidden_units(self):\n",
    "        return self.embedding.get_shape()[1].value\n",
    "\n",
    "    def train(\n",
    "            self, \n",
    "            session, \n",
    "            training_factory,\n",
    "            parameters,\n",
    "            exit_criteria,\n",
    "            validation,\n",
    "            logging_interval,\n",
    "            directories):\n",
    "\n",
    "        epoch = 1\n",
    "        iteration = 0\n",
    "        state = None\n",
    "        summary = self.summary_writer(directories.summary, session)\n",
    "        validation_hist = []\n",
    "        \n",
    "        try:\n",
    "            # Enumerate over the training set until exit criteria are met.\n",
    "            while True:\n",
    "                epoch_cost = 0.0\n",
    "                epoch_iteration = 0\n",
    "                \n",
    "                # rest state for each epoch\n",
    "                state = session.run(self.reset_state)\n",
    "                \n",
    "                # Enumerate over a single epoch of the training set\n",
    "                for x, y, complete in training_factory.epoch(self.batch_size, self.time_steps):\n",
    "                    _, cost, state, iteration = session.run(\n",
    "                        [self.train_step, self.cost, self.next_state, self.iteration],\n",
    "                        feed_dict={\n",
    "                            self.input: x,\n",
    "                            self.targets: y,\n",
    "                            # pass previous epoch state\n",
    "                            self.state: state,\n",
    "                            self.learning_rate: parameters.learning_rate,\n",
    "                            self.keep_probability: parameters.keep_probability\n",
    "                        })\n",
    "                    epoch_cost += cost\n",
    "                    epoch_iteration += self.time_steps\n",
    "                    if self._interval(iteration, logging_interval):\n",
    "                        tf.logging.info(\n",
    "                            \"Epoch %d (%0.4f complete), Iteration %d: epoch training perplexity %0.4f\" %\n",
    "                            (epoch, complete, iteration, self.perplexity(epoch_cost, epoch_iteration)))\n",
    "                    \n",
    "                    if validation is not None and self._interval(iteration, validation.interval):\n",
    "                        validation_perplexity = self.test(session, validation.epoch_factory)\n",
    "                        self.store_validation_perplexity(session, summary, iteration, validation_perplexity)\n",
    "                        tf.logging.info(\n",
    "                            \"Epoch %d, Iteration %d: validation perplexity %0.4f\" %\n",
    "                            (epoch, iteration, validation_perplexity))\n",
    "                        # save model if improved\n",
    "                        validation_hist.append(validation_perplexity)\n",
    "                        if (directories.model is not None) and (validation_perplexity == min(validation_hist)):\n",
    "                            model_filename = self._model_file(directories.model)\n",
    "                            tf.train.Saver().save(session, model_filename)\n",
    "                            self._write_model_parameters(directories.model)\n",
    "                            tf.logging.info(\"Saved model in %s \" % directories.model)\n",
    "                        \n",
    "                    if exit_criteria.max_iterations is not None and iteration > exit_criteria.max_iterations:\n",
    "                        raise StopTrainingException()\n",
    "\n",
    "                self.store_training_epoch_perplexity(\n",
    "                    session, summary, iteration, self.perplexity(epoch_cost, epoch_iteration))\n",
    "                epoch += 1\n",
    "                if exit_criteria.max_epochs is not None and epoch > exit_criteria.max_epochs:\n",
    "                    raise StopTrainingException()\n",
    "        except (StopTrainingException, KeyboardInterrupt):\n",
    "            pass\n",
    "        \n",
    "        tf.logging.info(\"Stop training at epoch %d, iteration %d\" % (epoch, iteration))\n",
    "        summary.close()\n",
    "\n",
    "    def _write_model_parameters(self, model_directory):\n",
    "        parameters = {\n",
    "            \"max_gradient\": self.max_gradient,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"time_steps\": self.time_steps,\n",
    "            \"vocabulary_size\": self.vocabulary_size,\n",
    "            \"hidden_units\": self.hidden_units,\n",
    "            \"layers\": self.layers\n",
    "        }\n",
    "        with open(self._parameters_file(model_directory), \"w\") as f:\n",
    "            json.dump(parameters, f, indent=4)\n",
    "\n",
    "    def test(self, session, epoch_factory):\n",
    "        state = session.run(self.reset_state)\n",
    "        epoch_cost = 0.0\n",
    "        epoch_iteration = 0\n",
    "        epoch = epoch_factory.epoch(self.batch_size, self.time_steps)\n",
    "        for x, y, _ in epoch:\n",
    "            cost, state = session.run(\n",
    "                [self.cost, self.next_state],\n",
    "                feed_dict={\n",
    "                    self.input: x, \n",
    "                    self.targets: y,\n",
    "                    self.state: state,\n",
    "                    self.keep_probability: 1.0\n",
    "                }\n",
    "            )\n",
    "            epoch_cost += cost\n",
    "            epoch_iteration += self.time_steps\n",
    "        return self.perplexity(epoch_cost, epoch_iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interval(iteration, interval):\n",
    "        return interval is not None and iteration > 1 and iteration % interval == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(cost, iterations):\n",
    "        return np.exp(cost / iterations)\n",
    "\n",
    "    def store_validation_perplexity(self, session, summary, iteration, validation_perplexity):\n",
    "        session.run(self.validation_perplexity.assign(validation_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    def store_training_epoch_perplexity(self, session, summary, iteration, training_perplexity):\n",
    "        session.run(self.training_epoch_perplexity.assign(training_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def summary_writer(summary_directory, session):\n",
    "        class NullSummaryWriter(object):\n",
    "            def add_summary(self, *args, **kwargs):\n",
    "                pass\n",
    "\n",
    "            def flush(self):\n",
    "                pass\n",
    "\n",
    "            def close(self):\n",
    "                pass\n",
    "\n",
    "        if summary_directory is not None:\n",
    "            return tf.summary.FileWriter(summary_directory, session.graph)\n",
    "        else:\n",
    "            return NullSummaryWriter()\n",
    "\n",
    "\n",
    "class StopTrainingException(Exception):\n",
    "    pass\n",
    "\n",
    "class ExitCriteria(object):\n",
    "    def __init__(self, max_iterations, max_epochs):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "class Parameters(object):\n",
    "    def __init__(self, learning_rate, keep_probability):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_probability = keep_probability\n",
    "\n",
    "class Validation(object):\n",
    "    def __init__(self, interval, epoch_factory):\n",
    "        self.interval = interval\n",
    "        self.epoch_factory = epoch_factory\n",
    "\n",
    "class Directories(object):\n",
    "    def __init__(self, model, summary):\n",
    "        self.model = model\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I spent quite a bit of time trying to get large eight-layer RNN to work until I realise LSTM cells don't need to be stacked into deep network. Two cell network cworks just fine and trains much faster and delivers better perplexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001E932792D30>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "        \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    rnn  = RNN(\n",
    "        max_gradient = 5, \n",
    "        batch_size = 64,\n",
    "        time_steps = 8, \n",
    "        # Add vocabulary slots of out of vocabulary (index 1) and padding (index 0).\n",
    "        vocabulary_size = len(shakespeare_vocabulary) + 2, \n",
    "        hidden_units = 512, \n",
    "        layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Epoch 1 (0.0367 complete), Iteration 50: epoch training perplexity 13737.4680\n",
      "INFO:tensorflow:Epoch 1 (0.0742 complete), Iteration 100: epoch training perplexity 6283.2805\n",
      "INFO:tensorflow:Epoch 1, Iteration 100: validation perplexity 2949.1785\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.1117 complete), Iteration 150: epoch training perplexity 5271.8605\n",
      "INFO:tensorflow:Epoch 1 (0.1492 complete), Iteration 200: epoch training perplexity 5264.4157\n",
      "INFO:tensorflow:Epoch 1, Iteration 200: validation perplexity 1089.7462\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.1867 complete), Iteration 250: epoch training perplexity 3834.2794\n",
      "INFO:tensorflow:Epoch 1 (0.2241 complete), Iteration 300: epoch training perplexity 3075.2769\n",
      "INFO:tensorflow:Epoch 1, Iteration 300: validation perplexity 945.0826\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.2616 complete), Iteration 350: epoch training perplexity 2590.5436\n",
      "INFO:tensorflow:Epoch 1 (0.2991 complete), Iteration 400: epoch training perplexity 2259.0028\n",
      "INFO:tensorflow:Epoch 1, Iteration 400: validation perplexity 863.6573\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.3366 complete), Iteration 450: epoch training perplexity 2018.8519\n",
      "INFO:tensorflow:Epoch 1 (0.3741 complete), Iteration 500: epoch training perplexity 1831.5537\n",
      "INFO:tensorflow:Epoch 1, Iteration 500: validation perplexity 815.0140\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.4115 complete), Iteration 550: epoch training perplexity 1694.8752\n",
      "INFO:tensorflow:Epoch 1 (0.4490 complete), Iteration 600: epoch training perplexity 1582.2618\n",
      "INFO:tensorflow:Epoch 1, Iteration 600: validation perplexity 776.7610\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.4865 complete), Iteration 650: epoch training perplexity 1493.5875\n",
      "INFO:tensorflow:Epoch 1 (0.5240 complete), Iteration 700: epoch training perplexity 1419.3268\n",
      "INFO:tensorflow:Epoch 1, Iteration 700: validation perplexity 754.5233\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.5615 complete), Iteration 750: epoch training perplexity 1355.2532\n",
      "INFO:tensorflow:Epoch 1 (0.5990 complete), Iteration 800: epoch training perplexity 1297.7604\n",
      "INFO:tensorflow:Epoch 1, Iteration 800: validation perplexity 724.1471\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.6364 complete), Iteration 850: epoch training perplexity 1248.9187\n",
      "INFO:tensorflow:Epoch 1 (0.6739 complete), Iteration 900: epoch training perplexity 1209.9439\n",
      "INFO:tensorflow:Epoch 1, Iteration 900: validation perplexity 707.1279\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.7114 complete), Iteration 950: epoch training perplexity 1176.4583\n",
      "INFO:tensorflow:Epoch 1 (0.7489 complete), Iteration 1000: epoch training perplexity 1143.4555\n",
      "INFO:tensorflow:Epoch 1, Iteration 1000: validation perplexity 691.2119\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.7864 complete), Iteration 1050: epoch training perplexity 1111.6441\n",
      "INFO:tensorflow:Epoch 1 (0.8238 complete), Iteration 1100: epoch training perplexity 1084.2082\n",
      "INFO:tensorflow:Epoch 1, Iteration 1100: validation perplexity 673.2454\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.8613 complete), Iteration 1150: epoch training perplexity 1057.6095\n",
      "INFO:tensorflow:Epoch 1 (0.8988 complete), Iteration 1200: epoch training perplexity 1033.4359\n",
      "INFO:tensorflow:Epoch 1, Iteration 1200: validation perplexity 660.0759\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 1 (0.9363 complete), Iteration 1250: epoch training perplexity 1011.9994\n",
      "INFO:tensorflow:Epoch 1 (0.9738 complete), Iteration 1300: epoch training perplexity 995.3992\n",
      "INFO:tensorflow:Epoch 1, Iteration 1300: validation perplexity 648.7899\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.0112 complete), Iteration 1350: epoch training perplexity 540.9890\n",
      "INFO:tensorflow:Epoch 2 (0.0487 complete), Iteration 1400: epoch training perplexity 538.3239\n",
      "INFO:tensorflow:Epoch 2, Iteration 1400: validation perplexity 635.8325\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.0862 complete), Iteration 1450: epoch training perplexity 520.7033\n",
      "INFO:tensorflow:Epoch 2 (0.1237 complete), Iteration 1500: epoch training perplexity 529.4575\n",
      "INFO:tensorflow:Epoch 2, Iteration 1500: validation perplexity 629.2473\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.1612 complete), Iteration 1550: epoch training perplexity 517.9625\n",
      "INFO:tensorflow:Epoch 2 (0.1987 complete), Iteration 1600: epoch training perplexity 510.9373\n",
      "INFO:tensorflow:Epoch 2, Iteration 1600: validation perplexity 620.3291\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.2361 complete), Iteration 1650: epoch training perplexity 503.2478\n",
      "INFO:tensorflow:Epoch 2 (0.2736 complete), Iteration 1700: epoch training perplexity 501.0452\n",
      "INFO:tensorflow:Epoch 2, Iteration 1700: validation perplexity 617.7392\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.3111 complete), Iteration 1750: epoch training perplexity 498.2286\n",
      "INFO:tensorflow:Epoch 2 (0.3486 complete), Iteration 1800: epoch training perplexity 494.6138\n",
      "INFO:tensorflow:Epoch 2, Iteration 1800: validation perplexity 613.9923\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.3861 complete), Iteration 1850: epoch training perplexity 493.1186\n",
      "INFO:tensorflow:Epoch 2 (0.4235 complete), Iteration 1900: epoch training perplexity 490.4319\n",
      "INFO:tensorflow:Epoch 2, Iteration 1900: validation perplexity 605.1690\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.4610 complete), Iteration 1950: epoch training perplexity 489.5386\n",
      "INFO:tensorflow:Epoch 2 (0.4985 complete), Iteration 2000: epoch training perplexity 488.7429\n",
      "INFO:tensorflow:Epoch 2, Iteration 2000: validation perplexity 595.8394\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.5360 complete), Iteration 2050: epoch training perplexity 488.7584\n",
      "INFO:tensorflow:Epoch 2 (0.5735 complete), Iteration 2100: epoch training perplexity 488.0027\n",
      "INFO:tensorflow:Epoch 2, Iteration 2100: validation perplexity 592.8255\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.6109 complete), Iteration 2150: epoch training perplexity 485.1709\n",
      "INFO:tensorflow:Epoch 2 (0.6484 complete), Iteration 2200: epoch training perplexity 485.0234\n",
      "INFO:tensorflow:Epoch 2, Iteration 2200: validation perplexity 584.5360\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.6859 complete), Iteration 2250: epoch training perplexity 485.2679\n",
      "INFO:tensorflow:Epoch 2 (0.7234 complete), Iteration 2300: epoch training perplexity 486.1504\n",
      "INFO:tensorflow:Epoch 2, Iteration 2300: validation perplexity 580.1102\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.7609 complete), Iteration 2350: epoch training perplexity 485.5399\n",
      "INFO:tensorflow:Epoch 2 (0.7984 complete), Iteration 2400: epoch training perplexity 484.0055\n",
      "INFO:tensorflow:Epoch 2, Iteration 2400: validation perplexity 575.6594\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.8358 complete), Iteration 2450: epoch training perplexity 482.8140\n",
      "INFO:tensorflow:Epoch 2 (0.8733 complete), Iteration 2500: epoch training perplexity 480.5726\n",
      "INFO:tensorflow:Epoch 2, Iteration 2500: validation perplexity 571.6059\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.9108 complete), Iteration 2550: epoch training perplexity 479.7742\n",
      "INFO:tensorflow:Epoch 2 (0.9483 complete), Iteration 2600: epoch training perplexity 479.3508\n",
      "INFO:tensorflow:Epoch 2, Iteration 2600: validation perplexity 566.3585\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 2 (0.9858 complete), Iteration 2650: epoch training perplexity 479.0834\n",
      "INFO:tensorflow:Epoch 3 (0.0232 complete), Iteration 2700: epoch training perplexity 419.2329\n",
      "INFO:tensorflow:Epoch 3, Iteration 2700: validation perplexity 564.6947\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.0607 complete), Iteration 2750: epoch training perplexity 417.0314\n",
      "INFO:tensorflow:Epoch 3 (0.0982 complete), Iteration 2800: epoch training perplexity 408.9819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Epoch 3, Iteration 2800: validation perplexity 561.9999\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.1357 complete), Iteration 2850: epoch training perplexity 408.4225\n",
      "INFO:tensorflow:Epoch 3 (0.1732 complete), Iteration 2900: epoch training perplexity 401.7830\n",
      "INFO:tensorflow:Epoch 3, Iteration 2900: validation perplexity 561.6067\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.2106 complete), Iteration 2950: epoch training perplexity 395.7552\n",
      "INFO:tensorflow:Epoch 3 (0.2481 complete), Iteration 3000: epoch training perplexity 391.6246\n",
      "INFO:tensorflow:Epoch 3, Iteration 3000: validation perplexity 561.2267\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.2856 complete), Iteration 3050: epoch training perplexity 391.0923\n",
      "INFO:tensorflow:Epoch 3 (0.3231 complete), Iteration 3100: epoch training perplexity 388.9978\n",
      "INFO:tensorflow:Epoch 3, Iteration 3100: validation perplexity 561.5627\n",
      "INFO:tensorflow:Epoch 3 (0.3606 complete), Iteration 3150: epoch training perplexity 386.5465\n",
      "INFO:tensorflow:Epoch 3 (0.3981 complete), Iteration 3200: epoch training perplexity 386.3986\n",
      "INFO:tensorflow:Epoch 3, Iteration 3200: validation perplexity 557.5833\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.4355 complete), Iteration 3250: epoch training perplexity 384.8040\n",
      "INFO:tensorflow:Epoch 3 (0.4730 complete), Iteration 3300: epoch training perplexity 384.2693\n",
      "INFO:tensorflow:Epoch 3, Iteration 3300: validation perplexity 555.3937\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.5105 complete), Iteration 3350: epoch training perplexity 384.6267\n",
      "INFO:tensorflow:Epoch 3 (0.5480 complete), Iteration 3400: epoch training perplexity 384.6785\n",
      "INFO:tensorflow:Epoch 3, Iteration 3400: validation perplexity 554.8542\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.5855 complete), Iteration 3450: epoch training perplexity 383.9003\n",
      "INFO:tensorflow:Epoch 3 (0.6229 complete), Iteration 3500: epoch training perplexity 383.0043\n",
      "INFO:tensorflow:Epoch 3, Iteration 3500: validation perplexity 552.3360\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.6604 complete), Iteration 3550: epoch training perplexity 383.9015\n",
      "INFO:tensorflow:Epoch 3 (0.6979 complete), Iteration 3600: epoch training perplexity 384.0166\n",
      "INFO:tensorflow:Epoch 3, Iteration 3600: validation perplexity 546.9266\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.7354 complete), Iteration 3650: epoch training perplexity 384.9831\n",
      "INFO:tensorflow:Epoch 3 (0.7729 complete), Iteration 3700: epoch training perplexity 384.6999\n",
      "INFO:tensorflow:Epoch 3, Iteration 3700: validation perplexity 544.4363\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.8103 complete), Iteration 3750: epoch training perplexity 384.1287\n",
      "INFO:tensorflow:Epoch 3 (0.8478 complete), Iteration 3800: epoch training perplexity 383.5529\n",
      "INFO:tensorflow:Epoch 3, Iteration 3800: validation perplexity 545.2535\n",
      "INFO:tensorflow:Epoch 3 (0.8853 complete), Iteration 3850: epoch training perplexity 382.5481\n",
      "INFO:tensorflow:Epoch 3 (0.9228 complete), Iteration 3900: epoch training perplexity 382.6229\n",
      "INFO:tensorflow:Epoch 3, Iteration 3900: validation perplexity 543.1792\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 3 (0.9603 complete), Iteration 3950: epoch training perplexity 382.3219\n",
      "INFO:tensorflow:Epoch 3 (0.9978 complete), Iteration 4000: epoch training perplexity 382.6682\n",
      "INFO:tensorflow:Epoch 3, Iteration 4000: validation perplexity 541.3458\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 4 (0.0352 complete), Iteration 4050: epoch training perplexity 343.3859\n",
      "INFO:tensorflow:Epoch 4 (0.0727 complete), Iteration 4100: epoch training perplexity 343.1313\n",
      "INFO:tensorflow:Epoch 4, Iteration 4100: validation perplexity 545.8772\n",
      "INFO:tensorflow:Epoch 4 (0.1102 complete), Iteration 4150: epoch training perplexity 344.1751\n",
      "INFO:tensorflow:Epoch 4 (0.1477 complete), Iteration 4200: epoch training perplexity 338.9803\n",
      "INFO:tensorflow:Epoch 4, Iteration 4200: validation perplexity 543.8062\n",
      "INFO:tensorflow:Epoch 4 (0.1852 complete), Iteration 4250: epoch training perplexity 334.8401\n",
      "INFO:tensorflow:Epoch 4 (0.2226 complete), Iteration 4300: epoch training perplexity 329.6593\n",
      "INFO:tensorflow:Epoch 4, Iteration 4300: validation perplexity 547.5110\n",
      "INFO:tensorflow:Epoch 4 (0.2601 complete), Iteration 4350: epoch training perplexity 327.3540\n",
      "INFO:tensorflow:Epoch 4 (0.2976 complete), Iteration 4400: epoch training perplexity 326.3183\n",
      "INFO:tensorflow:Epoch 4, Iteration 4400: validation perplexity 550.3235\n",
      "INFO:tensorflow:Epoch 4 (0.3351 complete), Iteration 4450: epoch training perplexity 324.6150\n",
      "INFO:tensorflow:Epoch 4 (0.3726 complete), Iteration 4500: epoch training perplexity 322.9317\n",
      "INFO:tensorflow:Epoch 4, Iteration 4500: validation perplexity 546.9848\n",
      "INFO:tensorflow:Epoch 4 (0.4100 complete), Iteration 4550: epoch training perplexity 323.0885\n",
      "INFO:tensorflow:Epoch 4 (0.4475 complete), Iteration 4600: epoch training perplexity 322.1667\n",
      "INFO:tensorflow:Epoch 4, Iteration 4600: validation perplexity 547.4769\n",
      "INFO:tensorflow:Epoch 4 (0.4850 complete), Iteration 4650: epoch training perplexity 321.3070\n",
      "INFO:tensorflow:Epoch 4 (0.5225 complete), Iteration 4700: epoch training perplexity 321.6488\n",
      "INFO:tensorflow:Epoch 4, Iteration 4700: validation perplexity 548.3408\n",
      "INFO:tensorflow:Epoch 4 (0.5600 complete), Iteration 4750: epoch training perplexity 321.6715\n",
      "INFO:tensorflow:Epoch 4 (0.5975 complete), Iteration 4800: epoch training perplexity 320.5518\n",
      "INFO:tensorflow:Epoch 4, Iteration 4800: validation perplexity 548.2510\n",
      "INFO:tensorflow:Epoch 4 (0.6349 complete), Iteration 4850: epoch training perplexity 320.4579\n",
      "INFO:tensorflow:Epoch 4 (0.6724 complete), Iteration 4900: epoch training perplexity 321.2049\n",
      "INFO:tensorflow:Epoch 4, Iteration 4900: validation perplexity 546.0439\n",
      "INFO:tensorflow:Epoch 4 (0.7099 complete), Iteration 4950: epoch training perplexity 321.7060\n",
      "INFO:tensorflow:Epoch 4 (0.7474 complete), Iteration 5000: epoch training perplexity 322.1922\n",
      "INFO:tensorflow:Epoch 4, Iteration 5000: validation perplexity 546.9867\n",
      "INFO:tensorflow:Epoch 4 (0.7849 complete), Iteration 5050: epoch training perplexity 321.9287\n",
      "INFO:tensorflow:Epoch 4 (0.8223 complete), Iteration 5100: epoch training perplexity 321.7364\n",
      "INFO:tensorflow:Epoch 4, Iteration 5100: validation perplexity 543.6796\n",
      "INFO:tensorflow:Epoch 4 (0.8598 complete), Iteration 5150: epoch training perplexity 321.2447\n",
      "INFO:tensorflow:Epoch 4 (0.8973 complete), Iteration 5200: epoch training perplexity 320.9195\n",
      "INFO:tensorflow:Epoch 4, Iteration 5200: validation perplexity 542.8679\n",
      "INFO:tensorflow:Epoch 4 (0.9348 complete), Iteration 5250: epoch training perplexity 320.7625\n",
      "INFO:tensorflow:Epoch 4 (0.9723 complete), Iteration 5300: epoch training perplexity 321.0117\n",
      "INFO:tensorflow:Epoch 4, Iteration 5300: validation perplexity 539.8480\n",
      "INFO:tensorflow:Saved model in model \n",
      "INFO:tensorflow:Epoch 5 (0.0097 complete), Iteration 5350: epoch training perplexity 305.7125\n",
      "INFO:tensorflow:Epoch 5 (0.0472 complete), Iteration 5400: epoch training perplexity 300.4606\n",
      "INFO:tensorflow:Epoch 5, Iteration 5400: validation perplexity 542.8422\n",
      "INFO:tensorflow:Epoch 5 (0.0847 complete), Iteration 5450: epoch training perplexity 289.5696\n",
      "INFO:tensorflow:Epoch 5 (0.1222 complete), Iteration 5500: epoch training perplexity 293.2352\n",
      "INFO:tensorflow:Epoch 5, Iteration 5500: validation perplexity 553.6244\n",
      "INFO:tensorflow:Epoch 5 (0.1597 complete), Iteration 5550: epoch training perplexity 287.5783\n",
      "INFO:tensorflow:Epoch 5 (0.1972 complete), Iteration 5600: epoch training perplexity 284.8385\n",
      "INFO:tensorflow:Epoch 5, Iteration 5600: validation perplexity 553.9464\n",
      "INFO:tensorflow:Epoch 5 (0.2346 complete), Iteration 5650: epoch training perplexity 281.0828\n",
      "INFO:tensorflow:Epoch 5 (0.2721 complete), Iteration 5700: epoch training perplexity 279.6156\n",
      "INFO:tensorflow:Epoch 5, Iteration 5700: validation perplexity 557.6722\n",
      "INFO:tensorflow:Epoch 5 (0.3096 complete), Iteration 5750: epoch training perplexity 279.0217\n",
      "INFO:tensorflow:Epoch 5 (0.3471 complete), Iteration 5800: epoch training perplexity 276.6598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Epoch 5, Iteration 5800: validation perplexity 563.9709\n",
      "INFO:tensorflow:Epoch 5 (0.3846 complete), Iteration 5850: epoch training perplexity 276.2024\n",
      "INFO:tensorflow:Epoch 5 (0.4220 complete), Iteration 5900: epoch training perplexity 275.4784\n",
      "INFO:tensorflow:Epoch 5, Iteration 5900: validation perplexity 557.3276\n",
      "INFO:tensorflow:Epoch 5 (0.4595 complete), Iteration 5950: epoch training perplexity 275.3568\n",
      "INFO:tensorflow:Epoch 5 (0.4970 complete), Iteration 6000: epoch training perplexity 274.6174\n",
      "INFO:tensorflow:Epoch 5, Iteration 6000: validation perplexity 562.9613\n",
      "INFO:tensorflow:Epoch 5 (0.5345 complete), Iteration 6050: epoch training perplexity 275.1809\n",
      "INFO:tensorflow:Epoch 5 (0.5720 complete), Iteration 6100: epoch training perplexity 275.1695\n",
      "INFO:tensorflow:Epoch 5, Iteration 6100: validation perplexity 562.1719\n",
      "INFO:tensorflow:Epoch 5 (0.6094 complete), Iteration 6150: epoch training perplexity 274.0170\n",
      "INFO:tensorflow:Epoch 5 (0.6469 complete), Iteration 6200: epoch training perplexity 274.4371\n",
      "INFO:tensorflow:Epoch 5, Iteration 6200: validation perplexity 556.1441\n",
      "INFO:tensorflow:Epoch 5 (0.6844 complete), Iteration 6250: epoch training perplexity 275.0149\n",
      "INFO:tensorflow:Epoch 5 (0.7219 complete), Iteration 6300: epoch training perplexity 275.5446\n",
      "INFO:tensorflow:Epoch 5, Iteration 6300: validation perplexity 553.9877\n",
      "INFO:tensorflow:Epoch 5 (0.7594 complete), Iteration 6350: epoch training perplexity 275.9194\n",
      "INFO:tensorflow:Epoch 5 (0.7969 complete), Iteration 6400: epoch training perplexity 275.7634\n",
      "INFO:tensorflow:Epoch 5, Iteration 6400: validation perplexity 557.5120\n",
      "INFO:tensorflow:Epoch 5 (0.8343 complete), Iteration 6450: epoch training perplexity 275.6909\n",
      "INFO:tensorflow:Epoch 5 (0.8718 complete), Iteration 6500: epoch training perplexity 275.0531\n",
      "INFO:tensorflow:Epoch 5, Iteration 6500: validation perplexity 557.7239\n",
      "INFO:tensorflow:Epoch 5 (0.9093 complete), Iteration 6550: epoch training perplexity 275.1347\n",
      "INFO:tensorflow:Epoch 5 (0.9468 complete), Iteration 6600: epoch training perplexity 275.1640\n",
      "INFO:tensorflow:Epoch 5, Iteration 6600: validation perplexity 558.8013\n",
      "INFO:tensorflow:Epoch 5 (0.9843 complete), Iteration 6650: epoch training perplexity 275.3608\n",
      "INFO:tensorflow:Stop training at epoch 6, iteration 6670\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree('model', ignore_errors = True)\n",
    "os.makedirs('model')\n",
    "shutil.rmtree('summary', ignore_errors = True)\n",
    "os.makedirs('summary')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    rnn.train(\n",
    "        session = sess, \n",
    "        training_factory = train_fact, \n",
    "        parameters = Parameters(learning_rate = 0.001, keep_probability = 0.7),\n",
    "        exit_criteria = ExitCriteria(max_iterations = None, max_epochs = 5),\n",
    "        validation = Validation(interval = 100, epoch_factory = valid_fact), \n",
    "        logging_interval = 50, \n",
    "        directories = Directories('model', 'summary')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network started to overfit at some point but our clever system only saved the best model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate RNN model performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001E9393603C8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Restoring parameters from model\\model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    rnn = RNN.restore(sess, 'model')\n",
    "    rnn_test_perplexity = rnn.test(sess, test_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our RNN language model we've built baseline unigram model to compare the perplexity on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniGram(object):\n",
    "\n",
    "    def __init__(self, texts):\n",
    "        self.counter = collections.Counter(WordGenerator(texts))\n",
    "        self.corpus_size = sum(self.counter.values())\n",
    "\n",
    "    # word probability is the word freq divided by the corpus size\n",
    "    def word_proba(self, word):\n",
    "        freq = self.counter[word]\n",
    "        return float(freq)/float(self.corpus_size)\n",
    "    \n",
    "    # assumes independence\n",
    "    def sentence_proba(self, sentence):\n",
    "        word_probs = [self.word_proba(w) for w in sentence]\n",
    "        return reduce(lambda x, y: x*y, word_probs)\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        p = self.sentence_proba(sentence)\n",
    "        return pow(p, -1.0/float(len(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniGram = UniGram(shakespeare_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'in' is 0.013105\n",
      "Probability of 'delivering' is 0.000005\n",
      "Probability of 'my' is 0.014527\n",
      "Probability of 'son' is 0.000650\n",
      "Probability of 'from' is 0.003156\n",
      "Probability of 'me' is 0.009196\n",
      "Sentence probability is  1.8897761650936493e-17\n",
      "Sentence perplexity is  612.7240664727524\n"
     ]
    }
   ],
   "source": [
    "s = ['in', 'delivering', 'my', 'son', 'from', 'me']\n",
    "for w in s:\n",
    "    print(\"Probability of '%s' is %f\" % (w, uniGram.word_proba(w)))\n",
    "print('Sentence probability is ' , uniGram.sentence_proba(s))\n",
    "print('Sentence perplexity is ' , uniGram.perplexity(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate perplexity for each sentence in the test data and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_epoch = test_fact.epoch(batch_size = 1, time_steps = 8)\n",
    "pp = []\n",
    "for x, y, progress in test_epoch:\n",
    "    for s in x:\n",
    "        pp.append(uniGram.perplexity([shakespeare_words[i-2] for i in s]))\n",
    "uni_test_perplexity = np.mean(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model test perplexity  480.527656363\n",
      "Unigram model test perplexity  1437.10427054\n"
     ]
    }
   ],
   "source": [
    "print('RNN model test perplexity ', rnn_test_perplexity)\n",
    "print('Unigram model test perplexity ', uni_test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our RNN language model shows superior performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
