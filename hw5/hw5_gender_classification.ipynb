{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN based language model for the vShakespeare plays and poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recurrent neural network based language model\n",
    "](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alberto.garza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "#print(tf.__version__)\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first begin by de-serializing the data streams from our pickle files, which contain all texts from both Shakespeare's plays and poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_plays.pickle', 'rb') as handle:\n",
    "    speeches = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the speeches and poems and feed them into shakespeare_texts, which will hold a list of all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>In delivering my son from me, I bury a second ...</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERTRAM</td>\n",
       "      <td>And I in going, madam, weep o'er my father's d...</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>What hope is there of his majesty's amendment?</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>This young gentlewoman had a father,--O, that\\...</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>He was famous, sir, in his profession, and it ...</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               text  gender\n",
       "0  COUNTESS  In delivering my son from me, I bury a second ...  Female\n",
       "1   BERTRAM  And I in going, madam, weep o'er my father's d...    Male\n",
       "2  COUNTESS     What hope is there of his majesty's amendment?  Female\n",
       "3  COUNTESS  This young gentlewoman had a father,--O, that\\...  Female\n",
       "4  COUNTESS  He was famous, sir, in his profession, and it ...  Female"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pickle files\n",
    "shakespeare_texts = [s['speech_text'] for s in speeches]\n",
    "shakespeare_speakers = [s['speaker'] for s in speeches]\n",
    "\n",
    "#character/gender mapping\n",
    "characters = pd.read_csv('characters.txt', sep='\\t')\n",
    "\n",
    "#combine character with their lines\n",
    "shakes = pd.DataFrame(list(zip(shakespeare_speakers, shakespeare_texts)))\n",
    "shakes.columns = ['character', 'text']\n",
    "\n",
    "#join in character gender and drop lines where gender is missing\n",
    "shakes = pd.merge(shakes, characters, how='left', on=['character'])\n",
    "shakes = shakes[shakes.gender.notnull()]\n",
    "shakes = shakes.reset_index(drop=True)\n",
    "shakes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19661 documents\n",
      "2 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"%d documents\" % len(shakes.text))\n",
    "print(\"%d categories\" % len(shakes.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = shakes.gender\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n"
     ]
    }
   ],
   "source": [
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", \n",
    "              type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", \n",
    "              dest=\"minibatch\", \n",
    "              default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", \n",
    "              dest=\"use_idf\", \n",
    "              default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", \n",
    "              default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "\n",
    "op.add_option(\"--n-features\", \n",
    "              type=int, \n",
    "              default=10000,\n",
    "              help=\"Maximum number of features (dimensions) to extract from text.\")\n",
    "\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", \n",
    "              dest=\"verbose\", \n",
    "              default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 0.456000s\n",
      "n_samples: 19661, n_features: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', \n",
    "                                   alternate_sign=False,\n",
    "                                   norm=None, \n",
    "                                   binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, \n",
    "                                       norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                                 max_features=opts.n_features,\n",
    "                                 min_df=2, \n",
    "                                 stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "    \n",
    "X = vectorizer.fit_transform(shakes.text)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "        init_size=1000, max_iter=100, max_no_improvement=10, n_clusters=2,\n",
      "        n_init=1, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=False)\n",
      "done in 0.072s\n",
      "\n",
      "Homogeneity: 0.000\n",
      "Completeness: 0.000\n",
      "V-measure: 0.000\n",
      "Adjusted Rand-Index: 0.005\n",
      "Silhouette Coefficient: 0.011\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: thou sir shall good thee thy come ll know love\n",
      "Cluster 1: lord good ay ll shall did say noble know think\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, \n",
    "                         init='k-means++', \n",
    "                         n_init=1,\n",
    "                         init_size=1000, \n",
    "                         batch_size=1000, \n",
    "                         verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, \n",
    "                init='k-means++', \n",
    "                max_iter=100, \n",
    "                n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
