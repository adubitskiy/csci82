{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alberto.garza\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alberto.garza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import pickle\n",
    "import nltk.data\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import print_function\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first begin by de-serializing the data streams from our pickle files, which contain all texts from both Shakespeare's plays and poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_plays.pickle', 'rb') as handle:\n",
    "    speeches = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the speeches and poems and feed them into shakespeare_texts, which will hold a list of all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>In delivering my son from me, I bury a second ...</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERTRAM</td>\n",
       "      <td>And I in going, madam, weep o'er my father's d...</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>What hope is there of his majesty's amendment?</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>This young gentlewoman had a father,--O, that\\...</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COUNTESS</td>\n",
       "      <td>He was famous, sir, in his profession, and it ...</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               text  gender\n",
       "0  COUNTESS  In delivering my son from me, I bury a second ...  Female\n",
       "1   BERTRAM  And I in going, madam, weep o'er my father's d...    Male\n",
       "2  COUNTESS     What hope is there of his majesty's amendment?  Female\n",
       "3  COUNTESS  This young gentlewoman had a father,--O, that\\...  Female\n",
       "4  COUNTESS  He was famous, sir, in his profession, and it ...  Female"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pickle files\n",
    "shakespeare_texts = [s['speech_text'] for s in speeches]\n",
    "shakespeare_speakers = [s['speaker'] for s in speeches]\n",
    "\n",
    "#character/gender mapping\n",
    "characters = pd.read_csv('characters.txt', sep='\\t')\n",
    "\n",
    "#combine character with their lines\n",
    "shakes = pd.DataFrame(list(zip(shakespeare_speakers, shakespeare_texts)))\n",
    "shakes.columns = ['character', 'text']\n",
    "\n",
    "#join in character gender and drop lines where gender is missing\n",
    "shakes = pd.merge(shakes, characters, how='left', on=['character'])\n",
    "shakes = shakes[shakes.gender.notnull()]\n",
    "shakes = shakes.reset_index(drop=True)\n",
    "shakes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19661 documents\n",
      "2 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"%d documents\" % len(shakes.text))\n",
    "print(\"%d categories\" % len(shakes.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = shakes.gender\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 0.405603s\n",
      "n_samples: 19661, n_features: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_features = 10000\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "\n",
    "# Perform an IDF normalization on the output of HashingVectorizer\n",
    "hasher = HashingVectorizer(n_features=n_features,\n",
    "                           stop_words='english', \n",
    "                           #alternate_sign=False,\n",
    "                           norm=None, \n",
    "                           binary=False)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                                 max_features=100,\n",
    "                                 min_df=2, \n",
    "                                 stop_words='english')\n",
    "   \n",
    "X = vectorizer.fit_transform(shakes.text)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.561604s\n",
      "Explained variance of the SVD step: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(99)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Summary:\n",
      "Homogeneity: 0.000\n",
      "Completeness: 0.000\n",
      "V-measure: 0.000\n",
      "Adjusted Rand-Index: -0.007\n",
      "Silhouette Coefficient: 0.105\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: thou shall good thee ll thy come let love man\n",
      "Cluster 1: sir lord good shall ay know did come ll pray\n",
      "\n",
      "\n",
      "Minibatch K-Means Summary:\n",
      "Homogeneity: 0.000\n",
      "Completeness: 0.000\n",
      "V-measure: 0.000\n",
      "Adjusted Rand-Index: -0.002\n",
      "Silhouette Coefficient: 0.110\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: thou thy thee art st hast dost say know come\n",
      "Cluster 1: lord sir good shall ll come let love know man\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "mbkm = MiniBatchKMeans(n_clusters=true_k, \n",
    "                     init='k-means++', \n",
    "                     n_init=1,\n",
    "                     init_size=500, \n",
    "                     batch_size=1000, \n",
    "                     verbose=False)\n",
    "\n",
    "km = KMeans(n_clusters=true_k, \n",
    "            init='k-means++', \n",
    "            max_iter=100, \n",
    "            n_init=1,\n",
    "            verbose=False)\n",
    "\n",
    "#fitting\n",
    "km.fit(X)\n",
    "mbkm.fit(X)\n",
    "\n",
    "print(\"K-Means Summary:\")\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "print()\n",
    "print(\"Top terms per cluster:\")\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()\n",
    "    \n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Minibatch K-Means Summary:\")\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, mbkm.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, mbkm.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, mbkm.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, mbkm.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, mbkm.labels_, sample_size=1000))\n",
    "print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "original_space_centroids = svd.inverse_transform(mbkm.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
