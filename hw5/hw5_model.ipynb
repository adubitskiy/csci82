{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN based language model for the vShakespeare plays and poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recurrent neural network based language model\n",
    "](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adubitskiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_plays.pickle', 'rb') as handle:\n",
    "    speeches = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('shakespeare_poetries.pickle', 'rb') as handle:\n",
    "    poetries = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare_texts = [s['speech_text'] for s in speeches] + [p['text'] for p in poetries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In delivering my son from me, I bury a second husband.', \"And I in going, madam, weep o'er my father's death\\nanew: but I must attend his majesty's command, to\\nwhom I am now in ward, evermore in subjection.\", 'You shall find of the king a husband, madam; you,\\nsir, a father: he that so generally is at all times\\ngood must of necessity hold his virtue to you; whose\\nworthiness would stir it up where it wanted rather\\nthan lack it where there is such abundance.', \"What hope is there of his majesty's amendment?\", 'He hath abandoned his physicians, madam; under whose\\npractises he hath persecuted time with hope, and\\nfinds no other advantage in the process but only the\\nlosing of hope by time.', \"This young gentlewoman had a father,--O, that\\n'had'! how sad a passage 'tis!--whose skill was\\nalmost as great as his honesty; had it stretched so\\nfar, would have made nature immortal, and death\\nshould have play for lack of work. Would, for the\\nking's sake, he were living! I think it would be\\nthe death of the king's disease.\", 'How called you the man you speak of, madam?', 'He was famous, sir, in his profession, and it was\\nhis great right to be so: Gerard de Narbon.', 'He was excellent indeed, madam: the king very\\nlately spoke of him admiringly and mourningly: he\\nwas skilful enough to have lived still, if knowledge\\ncould be set up against mortality.', 'What is it, my good lord, the king languishes of?']\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __iter__(self):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        for text in self.texts:\n",
    "            for s in tokenizer.tokenize(text):\n",
    "                yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In delivering my son from me, I bury a second husband.\n",
      "----------------------------------------\n",
      "And I in going, madam, weep o'er my father's death\n",
      "anew: but I must attend his majesty's command, to\n",
      "whom I am now in ward, evermore in subjection.\n",
      "----------------------------------------\n",
      "You shall find of the king a husband, madam; you,\n",
      "sir, a father: he that so generally is at all times\n",
      "good must of necessity hold his virtue to you; whose\n",
      "worthiness would stir it up where it wanted rather\n",
      "than lack it where there is such abundance.\n",
      "----------------------------------------\n",
      "What hope is there of his majesty's amendment?\n",
      "----------------------------------------\n",
      "He hath abandoned his physicians, madam; under whose\n",
      "practises he hath persecuted time with hope, and\n",
      "finds no other advantage in the process but only the\n",
      "losing of hope by time.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(SentenceGenerator(shakespeare_texts), 5):\n",
    "    print(s)\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordGenerator(object):\n",
    "    \n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __iter__(self):\n",
    "        trans = str.maketrans('','', string.punctuation)\n",
    "\n",
    "        for s in SentenceGenerator(self.texts):\n",
    "            for w in s.translate(trans).split():\n",
    "                yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "delivering\n",
      "my\n",
      "son\n",
      "from\n",
      "me\n",
      "I\n",
      "bury\n",
      "a\n",
      "second\n",
      "husband\n",
      "And\n",
      "I\n",
      "in\n",
      "going\n",
      "madam\n",
      "weep\n",
      "oer\n",
      "my\n",
      "fathers\n"
     ]
    }
   ],
   "source": [
    "for w in itertools.islice(WordGenerator(shakespeare_texts), 20):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(texts):\n",
    "    wordGen = WordGenerator(texts)\n",
    "    counter = collections.Counter(wordGen)\n",
    "    # unique list of words with the frequencies\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # unique list of words\n",
    "    words, x = list(zip(*count_pairs))\n",
    "    # reserve 0 for padding, 1 for out of vocabulary\n",
    "    start_index = 2\n",
    "    return words, dict(zip(words, range(start_index, len(words) + start_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare vocabulary size: 32006\n"
     ]
    }
   ],
   "source": [
    "shakespeare_words, shakespeare_vocabulary = build_vocabulary(shakespeare_texts)\n",
    "print('Shakespeare vocabulary size:', len(shakespeare_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8920 transport\n"
     ]
    }
   ],
   "source": [
    "i = shakespeare_vocabulary['transport']\n",
    "print(i, shakespeare_words[i- 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddedSentenceGenerator(object):\n",
    "\n",
    "    def __init__(self, texts, vocabulary):\n",
    "        self.texts = texts\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        trans = str.maketrans('','', string.punctuation)\n",
    "\n",
    "        for s in SentenceGenerator(self.texts):\n",
    "            yield [ self.vocabulary[w] if w in self.vocabulary else 1 for w in s.translate(trans).split()]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109, 11554, 9, 213, 50, 14, 3, 2259, 8, 877, 334]\n",
      "----------------------------------------\n",
      "[16, 3, 10, 902, 312, 599, 502, 9, 406, 152, 5601, 31, 3, 92, 823, 18, 4998, 646, 5, 303, 3, 57, 60, 10, 4277, 3047, 10, 7033]\n",
      "----------------------------------------\n",
      "[86, 39, 215, 6, 2, 134, 8, 334, 312, 7, 64, 8, 159, 24, 12, 30, 4968, 11, 52, 34, 496, 49, 92, 6, 2035, 285, 18, 512, 5, 7, 269, 3669, 55, 1130, 15, 120, 158, 15, 7899, 353, 90, 887, 15, 158, 93, 11, 99, 4877]\n",
      "----------------------------------------\n",
      "[56, 313, 11, 93, 6, 18, 4998, 11185]\n",
      "----------------------------------------\n",
      "[104, 72, 13996, 18, 5366, 312, 526, 269, 4738, 24, 72, 27863, 121, 17, 313, 4, 1735, 43, 179, 1239, 10, 2, 3479, 31, 392, 2, 3463, 6, 313, 40, 121]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary), 5):\n",
    "    print(s)\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we could recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'delivering', 'my', 'son', 'from', 'me', 'I', 'bury', 'a', 'second', 'husband']\n",
      "----------------------------------------\n",
      "['And', 'I', 'in', 'going', 'madam', 'weep', 'oer', 'my', 'fathers', 'death', 'anew', 'but', 'I', 'must', 'attend', 'his', 'majestys', 'command', 'to', 'whom', 'I', 'am', 'now', 'in', 'ward', 'evermore', 'in', 'subjection']\n",
      "----------------------------------------\n",
      "['You', 'shall', 'find', 'of', 'the', 'king', 'a', 'husband', 'madam', 'you', 'sir', 'a', 'father', 'he', 'that', 'so', 'generally', 'is', 'at', 'all', 'times', 'good', 'must', 'of', 'necessity', 'hold', 'his', 'virtue', 'to', 'you', 'whose', 'worthiness', 'would', 'stir', 'it', 'up', 'where', 'it', 'wanted', 'rather', 'than', 'lack', 'it', 'where', 'there', 'is', 'such', 'abundance']\n",
      "----------------------------------------\n",
      "['What', 'hope', 'is', 'there', 'of', 'his', 'majestys', 'amendment']\n",
      "----------------------------------------\n",
      "['He', 'hath', 'abandoned', 'his', 'physicians', 'madam', 'under', 'whose', 'practises', 'he', 'hath', 'persecuted', 'time', 'with', 'hope', 'and', 'finds', 'no', 'other', 'advantage', 'in', 'the', 'process', 'but', 'only', 'the', 'losing', 'of', 'hope', 'by', 'time']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in itertools.islice(EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary), 5):\n",
    "    print([shakespeare_words[i-2] for i in s])\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# factory for creating the epochs\n",
    "class EpochFactory(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.raw_data = [word for sentence in sentences for word in sentence]\n",
    "\n",
    "    def epoch(self, batch_size, time_steps):\n",
    "        return Epoch(self.raw_data, batch_size, time_steps)\n",
    "    \n",
    "# provides one epoch worth of data \n",
    "class Epoch(object):\n",
    "    def __init__(self, words, batch_size, time_steps):\n",
    "        self.raw_data = words\n",
    "        self.batch_size = batch_size\n",
    "        self.time_steps = time_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "\n",
    "        data_len = np.size(self.raw_data)\n",
    "        batch_len = data_len // self.batch_size\n",
    "        chunk_len = (batch_len - 1) // self.time_steps\n",
    "\n",
    "        assert (chunk_len > 0), \"chunk_len == 0, decrease batch_size or num_steps\"\n",
    "\n",
    "        data = np.reshape(self.raw_data[0 : self.batch_size * batch_len], [self.batch_size, batch_len])\n",
    "        \n",
    "        for i in range(chunk_len):\n",
    "            x = data[0 : self.batch_size , i * self.time_steps     : (i + 1) * self.time_steps] \n",
    "            y = data[0 : self.batch_size , i * self.time_steps + 1 : (i + 1) * self.time_steps + 1]\n",
    "            yield x, y, float(i)/float(chunk_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 51693 sentences\n"
     ]
    }
   ],
   "source": [
    "shakespeare_sentences = [s for s in EmbeddedSentenceGenerator(shakespeare_texts, shakespeare_vocabulary)]\n",
    "print('Total %d sentences'%len(shakespeare_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fact = EpochFactory(shakespeare_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Inputs----------------\n",
      "[109, 11554, 9, 213, 50, 14, 3, 2259, 8, 877, 334, 16]\n",
      "[1017, 5, 26765, 52, 61, 15, 89, 19, 20975, 274, 2, 1668]\n",
      "[7, 37, 81, 83, 805, 159, 85, 776, 2810, 19, 28, 159]\n",
      "[11, 939, 290, 1423, 12, 3, 1020, 16, 31430, 10, 8, 695]\n",
      "[8, 3715, 110, 302, 7, 1773, 16117, 21783, 3400, 11620, 15794, 8610]\n",
      "[6, 693, 2053, 13926, 2999, 35, 948, 2602, 3258, 141, 3, 58]\n",
      "[30, 46, 47, 4137, 378, 7, 5, 23, 17, 77, 255, 7]\n",
      "[136, 14615, 8, 1576, 8, 438, 8, 569, 69, 209, 33, 150]\n",
      "---------------Targets---------------\n",
      "[11554, 9, 213, 50, 14, 3, 2259, 8, 877, 334, 16, 3]\n",
      "[5, 26765, 52, 61, 15, 89, 19, 20975, 274, 2, 1668, 140]\n",
      "[37, 81, 83, 805, 159, 85, 776, 2810, 19, 28, 159, 51]\n",
      "[939, 290, 1423, 12, 3, 1020, 16, 31430, 10, 8, 695, 29]\n",
      "[3715, 110, 302, 7, 1773, 16117, 21783, 3400, 11620, 15794, 8610, 16920]\n",
      "[693, 2053, 13926, 2999, 35, 948, 2602, 3258, 141, 3, 58, 843]\n",
      "[46, 47, 4137, 378, 7, 5, 23, 17, 77, 255, 7, 19]\n",
      "[14615, 8, 1576, 8, 438, 8, 569, 69, 209, 33, 150, 80]\n"
     ]
    }
   ],
   "source": [
    "epoch = fact.epoch(batch_size = 8, time_steps = 12)\n",
    "for x, y, progress in itertools.islice(epoch, 1):\n",
    "    print('---------------Inputs----------------')\n",
    "    for s in x:\n",
    "        print([i for i in s])\n",
    "    print('---------------Targets---------------')\n",
    "    for s in y:\n",
    "        print([i for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample chunk recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Inputs----------------\n",
      "['In', 'delivering', 'my', 'son', 'from', 'me', 'I', 'bury', 'a', 'second', 'husband', 'And']\n",
      "['int', 'to', 'mete', 'at', 'if', 'it', 'may', 'be', 'Wide', 'o', 'the', 'bow']\n",
      "['you', 'are', 'like', 'an', 'honourable', 'father', 'If', 'Signior', 'Leonato', 'be', 'her', 'father']\n",
      "['is', 'fine', 'full', 'perfect', 'that', 'I', 'taste', 'And', 'violenteth', 'in', 'a', 'sense']\n",
      "['a', 'bachelor', 'How', 'answer', 'you', 'la', 'plus', 'belle', 'Katharine', 'du', 'monde', 'mon']\n",
      "['of', 'King', 'Henry', 'VI', 'Ghost', 'To', 'KING', 'RICHARD', 'III', 'When', 'I', 'was']\n",
      "['so', 'But', 'what', 'compact', 'mean', 'you', 'to', 'have', 'with', 'us', 'Will', 'you']\n",
      "['Thou', 'counterfeitst', 'a', 'bark', 'a', 'sea', 'a', 'wind', 'For', 'still', 'thy', 'eyes']\n",
      "---------------Targets---------------\n",
      "['delivering', 'my', 'son', 'from', 'me', 'I', 'bury', 'a', 'second', 'husband', 'And', 'I']\n",
      "['to', 'mete', 'at', 'if', 'it', 'may', 'be', 'Wide', 'o', 'the', 'bow', 'hand']\n",
      "['are', 'like', 'an', 'honourable', 'father', 'If', 'Signior', 'Leonato', 'be', 'her', 'father', 'she']\n",
      "['fine', 'full', 'perfect', 'that', 'I', 'taste', 'And', 'violenteth', 'in', 'a', 'sense', 'as']\n",
      "['bachelor', 'How', 'answer', 'you', 'la', 'plus', 'belle', 'Katharine', 'du', 'monde', 'mon', 'tres']\n",
      "['King', 'Henry', 'VI', 'Ghost', 'To', 'KING', 'RICHARD', 'III', 'When', 'I', 'was', 'mortal']\n",
      "['But', 'what', 'compact', 'mean', 'you', 'to', 'have', 'with', 'us', 'Will', 'you', 'be']\n",
      "['counterfeitst', 'a', 'bark', 'a', 'sea', 'a', 'wind', 'For', 'still', 'thy', 'eyes', 'which']\n"
     ]
    }
   ],
   "source": [
    "epoch = fact.epoch(batch_size = 8, time_steps = 12)\n",
    "for x, y, progress in itertools.islice(epoch, 1):\n",
    "    print('---------------Inputs----------------')\n",
    "    for s in x:\n",
    "        print([shakespeare_words[i-2] for i in s])\n",
    "    print('---------------Targets---------------')\n",
    "    for s in y:\n",
    "        print([shakespeare_words[i-2] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train, validation and test data sets\n",
    "We have ~ 51K speehes lets put 5% to test and 5% to validation. Let's make them continues chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test / valid sample size: 2584, valid start at 5925, test start at 30363\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "test_size = int(len(shakespeare_sentences)*0.05)\n",
    "start_valid = np.random.randint(0, len(shakespeare_sentences)/2 - test_size)\n",
    "end_valid = start_valid + test_size\n",
    "start_test = np.random.randint(len(shakespeare_sentences)/2, len(shakespeare_sentences) - test_size)\n",
    "end_test = start_test + test_size\n",
    "print('Test / valid sample size: %d, valid start at %d, test start at %d'%(test_size, start_valid, start_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has 46525, validation has 2584 and test has 2584 speeches\n"
     ]
    }
   ],
   "source": [
    "s = shakespeare_sentences\n",
    "train_sentences = s[:start_valid] + s[end_valid:start_test] + s[end_test:]\n",
    "valid_sentences = s[start_valid:end_valid]\n",
    "test_sentences = s[start_test:end_test]\n",
    "print(\n",
    "    'Training has %d, validation has %d and test has %d speeches'%\n",
    "    (len(train_sentences), len(valid_sentences), len(test_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fact = EpochFactory(train_sentences)\n",
    "valid_fact = EpochFactory(valid_sentences)\n",
    "train_fact = EpochFactory(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \n",
    "    @classmethod\n",
    "    def restore(cls, session, model_directory):\n",
    "        with open(cls._parameters_file(model_directory)) as f:\n",
    "            parameters = json.load(f)\n",
    "        model = cls(\n",
    "            parameters[\"max_gradient\"],\n",
    "            parameters[\"batch_size\"], \n",
    "            parameters[\"time_steps\"], \n",
    "            parameters[\"vocabulary_size\"],\n",
    "            parameters[\"hidden_units\"], \n",
    "            parameters[\"layers\"]\n",
    "        )\n",
    "        tf.train.Saver().restore(session, cls._model_file(model_directory))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _parameters_file(model_directory):\n",
    "        return os.path.join(model_directory, \"parameters.json\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _model_file(model_directory):\n",
    "        return os.path.join(model_directory, \"model\")\n",
    "\n",
    "    def __init__(self, max_gradient, batch_size, time_steps, vocabulary_size, hidden_units, layers):\n",
    "\n",
    "        self.max_gradient = max_gradient\n",
    "        self.layers = layers\n",
    "        # Add vocabulary slots of out of vocabulary (index 1) and padding (index 0).\n",
    "        vocabulary_size += 2\n",
    "\n",
    "        with tf.name_scope(\"Parameters\"):\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "            self.keep_probability = tf.placeholder(tf.float32, name=\"keep_probability\")\n",
    "\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input = tf.placeholder(tf.int32, shape=(batch_size, time_steps), name=\"input\")\n",
    "            self.targets = tf.placeholder(tf.int32, shape=(batch_size, time_steps), name=\"targets\")\n",
    "\n",
    "        with tf.name_scope(\"Embedding\"):\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform((vocabulary_size, hidden_units), -1.0, 1.0),\n",
    "                dtype=tf.float32,\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.input, name=\"embedded_input\")\n",
    "\n",
    "        with tf.name_scope(\"RNN\"):\n",
    "            # it is a bit harder to manage unconcatenated state\n",
    "            # for our purposes it should be OK to use concatenated state\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(hidden_units, state_is_tuple = False)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_probability)\n",
    "            rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * layers, state_is_tuple = False)\n",
    "            \n",
    "            self.reset_state = rnn_layers.zero_state(batch_size, dtype=tf.float32)\n",
    "            self.state = tf.placeholder(tf.float32, self.reset_state.get_shape(), \"state\")\n",
    "            \n",
    "            self.outputs, self.next_state = tf.nn.dynamic_rnn(\n",
    "                rnn_layers, self.embedded_input, initial_state=self.state, time_major=False)\n",
    "\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            # Concatenate all the batches into a single row.\n",
    "            self.flattened_outputs = tf.reshape(\n",
    "                tf.concat( self.outputs, 1),\n",
    "                (-1, hidden_units),\n",
    "                name=\"flattened_outputs\"\n",
    "            )\n",
    "            \n",
    "            # Project the outputs onto the vocabulary.\n",
    "            self.w = tf.get_variable(\n",
    "                \"w\", (hidden_units, vocabulary_size), initializer = tf.truncated_normal_initializer)\n",
    "            self.b = tf.get_variable(\n",
    "                \"b\", vocabulary_size, initializer = tf.truncated_normal_initializer)\n",
    "            self.predicted = tf.matmul(self.flattened_outputs, self.w) + self.b\n",
    "            \n",
    "            # Compare predictions to labels.\n",
    "            self.loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.predicted],\n",
    "                [tf.concat(self.targets, -1)],\n",
    "                [tf.ones(batch_size * time_steps)]\n",
    "            )\n",
    "            self.cost = tf.div(tf.reduce_sum(self.loss), batch_size, name=\"cost\")\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            self.validation_perplexity = tf.Variable(\n",
    "                dtype=tf.float32, initial_value=float(\"inf\"), trainable=False, name=\"validation_perplexity\")\n",
    "            tf.summary.scalar(self.validation_perplexity.op.name, self.validation_perplexity)\n",
    "            self.training_epoch_perplexity = tf.Variable(\n",
    "                dtype=tf.float32, initial_value=float(\"inf\"), trainable=False, name=\"training_epoch_perplexity\")\n",
    "            tf.summary.scalar(self.training_epoch_perplexity.op.name, self.training_epoch_perplexity)\n",
    "            self.iteration = tf.Variable(0, dtype=tf.int64, name=\"iteration\", trainable=False)\n",
    "            # gradient clipping\n",
    "            self.gradients, _ = tf.clip_by_global_norm(\n",
    "                tf.gradients(self.cost, tf.trainable_variables()), max_gradient, name=\"clip_gradients\")\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_step = optimizer.apply_gradients(\n",
    "                zip(self.gradients, tf.trainable_variables()), name=\"train_step\", global_step=self.iteration)\n",
    "\n",
    "        self.initialize = tf.global_variables_initializer()\n",
    "        self.summary = tf.summary.merge_all()\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.input.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def time_steps(self):\n",
    "        return self.input.get_shape()[1].value\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.embedding.get_shape()[0].value\n",
    "\n",
    "    @property\n",
    "    def hidden_units(self):\n",
    "        return self.embedding.get_shape()[1].value\n",
    "\n",
    "    def train(\n",
    "            self, \n",
    "            session, \n",
    "            training_factory,\n",
    "            parameters,\n",
    "            exit_criteria,\n",
    "            validation,\n",
    "            logging_interval,\n",
    "            directories):\n",
    "        epoch = 1\n",
    "        iteration = 0\n",
    "        state = None\n",
    "        summary = self.summary_writer(directories.summary, session)\n",
    "        try:\n",
    "            # Enumerate over the training set until exit criteria are met.\n",
    "            while True:\n",
    "                epoch_cost = 0.0\n",
    "                epoch_iteration = 0\n",
    "                \n",
    "                # rest state for each epoch\n",
    "                state = session.run(self.reset_state)\n",
    "                \n",
    "                # Enumerate over a single epoch of the training set\n",
    "                for x, y, complete in training_factory.epoch(self.batch_size, self.time_steps):\n",
    "                    _, cost, state, iteration = session.run(\n",
    "                        [self.train_step, self.cost, self.next_state, self.iteration],\n",
    "                        feed_dict={\n",
    "                            self.input: x,\n",
    "                            self.targets: y,\n",
    "                            # pass previous epoch state\n",
    "                            self.state: state,\n",
    "                            self.learning_rate: parameters.learning_rate,\n",
    "                            self.keep_probability: parameters.keep_probability\n",
    "                        })\n",
    "                    epoch_cost += cost\n",
    "                    epoch_iteration += self.time_steps\n",
    "                    if self._interval(iteration, logging_interval):\n",
    "                        tf.logging.info(\n",
    "                            \"Epoch %d (%0.4f complete), Iteration %d: epoch training perplexity %0.4f\" %\n",
    "                            (epoch, complete, iteration, self.perplexity(epoch_cost, epoch_iteration))\n",
    "                    )\n",
    "                    if validation is not None and self._interval(iteration, validation.interval):\n",
    "                        validation_perplexity = self.test(session, validation.epoch_factory)\n",
    "                        self.store_validation_perplexity(session, summary, iteration, validation_perplexity)\n",
    "                        tf.logging.info(\n",
    "                            \"Epoch %d, Iteration %d: validation perplexity %0.4f\" %\n",
    "                            (epoch, iteration, validation_perplexity)\n",
    "                    )\n",
    "                        \n",
    "                    if exit_criteria.max_iterations is not None and iteration > exit_criteria.max_iterations:\n",
    "                        raise StopTrainingException()\n",
    "\n",
    "                self.store_training_epoch_perplexity(\n",
    "                    session, summary, iteration, self.perplexity(epoch_cost, epoch_iteration))\n",
    "                epoch += 1\n",
    "                if exit_criteria.max_epochs is not None and epoch > exit_criteria.max_epochs:\n",
    "                    raise StopTrainingException()\n",
    "        except (StopTrainingException, KeyboardInterrupt):\n",
    "            pass\n",
    "        \n",
    "        tf.logging.info(\"Stop training at epoch %d, iteration %d\" % (epoch, iteration))\n",
    "        summary.close()\n",
    "        \n",
    "        if directories.model is not None:\n",
    "            model_filename = self._model_file(directories.model)\n",
    "            tf.train.Saver().save(session, model_filename)\n",
    "            self._write_model_parameters(directories.model)\n",
    "            tf.logging.info(\"Saved model in %s \" % directories.model)\n",
    "\n",
    "    def _write_model_parameters(self, model_directory):\n",
    "        parameters = {\n",
    "            \"max_gradient\": self.max_gradient,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"time_steps\": self.time_steps,\n",
    "            \"vocabulary_size\": self.vocabulary_size,\n",
    "            \"hidden_units\": self.hidden_units,\n",
    "            \"layers\": self.layers\n",
    "        }\n",
    "        with open(self._parameters_file(model_directory), \"w\") as f:\n",
    "            json.dump(parameters, f, indent=4)\n",
    "\n",
    "    def test(self, session, epoch_factory):\n",
    "        state = session.run(self.reset_state)\n",
    "        epoch_cost = epoch_iteration = 0\n",
    "        epoch = epoch_factory.epoch(self.batch_size, self.time_steps)\n",
    "        for x, y, _ in epoch:\n",
    "            cost, state = session.run(\n",
    "                [self.cost, self.next_state],\n",
    "                feed_dict={\n",
    "                    self.input: x, \n",
    "                    self.targets: y,\n",
    "                    self.state: state,\n",
    "                    self.keep_probability: 1.0\n",
    "                }\n",
    "            )\n",
    "            epoch_cost += cost\n",
    "            epoch_iteration += self.time_steps\n",
    "        return self.perplexity(epoch_cost, epoch_iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interval(iteration, interval):\n",
    "        return interval is not None and iteration > 1 and iteration % interval == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(cost, iterations):\n",
    "        return np.exp(cost / iterations)\n",
    "\n",
    "    def store_validation_perplexity(self, session, summary, iteration, validation_perplexity):\n",
    "        session.run(self.validation_perplexity.assign(validation_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    def store_training_epoch_perplexity(self, session, summary, iteration, training_perplexity):\n",
    "        session.run(self.training_epoch_perplexity.assign(training_perplexity))\n",
    "        summary.add_summary(session.run(self.summary), global_step=iteration)\n",
    "\n",
    "    @staticmethod\n",
    "    def summary_writer(summary_directory, session):\n",
    "        class NullSummaryWriter(object):\n",
    "            def add_summary(self, *args, **kwargs):\n",
    "                pass\n",
    "\n",
    "            def flush(self):\n",
    "                pass\n",
    "\n",
    "            def close(self):\n",
    "                pass\n",
    "\n",
    "        if summary_directory is not None:\n",
    "            return tf.summary.FileWriter(summary_directory, session.graph)\n",
    "        else:\n",
    "            return NullSummaryWriter()\n",
    "\n",
    "\n",
    "class StopTrainingException(Exception):\n",
    "    pass\n",
    "\n",
    "class ExitCriteria(object):\n",
    "    def __init__(self, max_iterations, max_epochs):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "class Parameters(object):\n",
    "    def __init__(self, learning_rate, keep_probability):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_probability = keep_probability\n",
    "\n",
    "class Validation(object):\n",
    "    def __init__(self, interval, epoch_factory):\n",
    "        self.interval = interval\n",
    "        self.epoch_factory = epoch_factory\n",
    "\n",
    "class Directories(object):\n",
    "    def __init__(self, model, summary):\n",
    "        self.model = model\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000023FD0556400>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "        \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    rnn  = RNN(\n",
    "        max_gradient = 5, \n",
    "        batch_size = 64, \n",
    "        time_steps = 20, \n",
    "        vocabulary_size = len(shakespeare_vocabulary), \n",
    "        hidden_units = 1024, \n",
    "        layers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Epoch 1 (0.3103 complete), Iteration 10: epoch training perplexity 2450761830.5296\n",
      "INFO:tensorflow:Epoch 1 (0.6552 complete), Iteration 20: epoch training perplexity 182315488.7319\n",
      "INFO:tensorflow:Epoch 2 (0.0000 complete), Iteration 30: epoch training perplexity 80689.6654\n",
      "INFO:tensorflow:Epoch 2 (0.3448 complete), Iteration 40: epoch training perplexity 22960.8340\n",
      "INFO:tensorflow:Epoch 2 (0.6897 complete), Iteration 50: epoch training perplexity 6356.4918\n",
      "INFO:tensorflow:Epoch 3 (0.0345 complete), Iteration 60: epoch training perplexity 1481.1359\n",
      "INFO:tensorflow:Epoch 3 (0.3793 complete), Iteration 70: epoch training perplexity 1317.4303\n",
      "INFO:tensorflow:Epoch 3 (0.7241 complete), Iteration 80: epoch training perplexity 1236.2928\n",
      "INFO:tensorflow:Epoch 4 (0.0690 complete), Iteration 90: epoch training perplexity 1174.3185\n",
      "INFO:tensorflow:Epoch 4 (0.4138 complete), Iteration 100: epoch training perplexity 1112.7382\n",
      "INFO:tensorflow:Epoch 4, Iteration 100: validation perplexity 1998.7114\n",
      "INFO:tensorflow:Epoch 4 (0.7586 complete), Iteration 110: epoch training perplexity 1076.4519\n",
      "INFO:tensorflow:Epoch 5 (0.1034 complete), Iteration 120: epoch training perplexity 1047.7945\n",
      "INFO:tensorflow:Epoch 5 (0.4483 complete), Iteration 130: epoch training perplexity 988.9653\n",
      "INFO:tensorflow:Epoch 5 (0.7931 complete), Iteration 140: epoch training perplexity 970.9579\n",
      "INFO:tensorflow:Epoch 6 (0.1379 complete), Iteration 150: epoch training perplexity 976.6010\n",
      "INFO:tensorflow:Epoch 6 (0.4828 complete), Iteration 160: epoch training perplexity 924.8562\n",
      "INFO:tensorflow:Epoch 6 (0.8276 complete), Iteration 170: epoch training perplexity 903.0259\n",
      "INFO:tensorflow:Epoch 7 (0.1724 complete), Iteration 180: epoch training perplexity 879.1380\n",
      "INFO:tensorflow:Epoch 7 (0.5172 complete), Iteration 190: epoch training perplexity 857.9981\n",
      "INFO:tensorflow:Epoch 7 (0.8621 complete), Iteration 200: epoch training perplexity 833.7921\n",
      "INFO:tensorflow:Epoch 7, Iteration 200: validation perplexity 2164.4519\n",
      "INFO:tensorflow:Epoch 8 (0.2069 complete), Iteration 210: epoch training perplexity 811.1218\n",
      "INFO:tensorflow:Epoch 8 (0.5517 complete), Iteration 220: epoch training perplexity 780.4781\n",
      "INFO:tensorflow:Epoch 8 (0.8966 complete), Iteration 230: epoch training perplexity 767.6271\n",
      "INFO:tensorflow:Epoch 9 (0.2414 complete), Iteration 240: epoch training perplexity 755.3028\n",
      "INFO:tensorflow:Epoch 9 (0.5862 complete), Iteration 250: epoch training perplexity 727.7219\n",
      "INFO:tensorflow:Epoch 9 (0.9310 complete), Iteration 260: epoch training perplexity 723.3951\n",
      "INFO:tensorflow:Epoch 10 (0.2759 complete), Iteration 270: epoch training perplexity 702.2025\n",
      "INFO:tensorflow:Epoch 10 (0.6207 complete), Iteration 280: epoch training perplexity 683.2837\n",
      "INFO:tensorflow:Epoch 10 (0.9655 complete), Iteration 290: epoch training perplexity 676.6717\n",
      "INFO:tensorflow:Epoch 11 (0.3103 complete), Iteration 300: epoch training perplexity 654.7141\n",
      "INFO:tensorflow:Epoch 11, Iteration 300: validation perplexity 2351.1608\n",
      "INFO:tensorflow:Epoch 11 (0.6552 complete), Iteration 310: epoch training perplexity 640.7840\n",
      "INFO:tensorflow:Epoch 12 (0.0000 complete), Iteration 320: epoch training perplexity 653.9928\n",
      "INFO:tensorflow:Epoch 12 (0.3448 complete), Iteration 330: epoch training perplexity 629.4333\n",
      "INFO:tensorflow:Epoch 12 (0.6897 complete), Iteration 340: epoch training perplexity 612.3923\n",
      "INFO:tensorflow:Epoch 13 (0.0345 complete), Iteration 350: epoch training perplexity 636.9226\n",
      "INFO:tensorflow:Epoch 13 (0.3793 complete), Iteration 360: epoch training perplexity 610.7873\n",
      "INFO:tensorflow:Epoch 13 (0.7241 complete), Iteration 370: epoch training perplexity 588.1610\n",
      "INFO:tensorflow:Epoch 14 (0.0690 complete), Iteration 380: epoch training perplexity 564.8934\n",
      "INFO:tensorflow:Epoch 14 (0.4138 complete), Iteration 390: epoch training perplexity 569.7530\n",
      "INFO:tensorflow:Epoch 14 (0.7586 complete), Iteration 400: epoch training perplexity 559.3346\n",
      "INFO:tensorflow:Epoch 14, Iteration 400: validation perplexity 2483.6168\n",
      "INFO:tensorflow:Epoch 15 (0.1034 complete), Iteration 410: epoch training perplexity 534.0044\n",
      "INFO:tensorflow:Epoch 15 (0.4483 complete), Iteration 420: epoch training perplexity 533.2174\n",
      "INFO:tensorflow:Epoch 15 (0.7931 complete), Iteration 430: epoch training perplexity 533.4477\n",
      "INFO:tensorflow:Epoch 16 (0.1379 complete), Iteration 440: epoch training perplexity 538.8395\n",
      "INFO:tensorflow:Epoch 16 (0.4828 complete), Iteration 450: epoch training perplexity 526.5729\n",
      "INFO:tensorflow:Epoch 16 (0.8276 complete), Iteration 460: epoch training perplexity 517.5377\n",
      "INFO:tensorflow:Epoch 17 (0.1724 complete), Iteration 470: epoch training perplexity 519.8956\n",
      "INFO:tensorflow:Epoch 17 (0.5172 complete), Iteration 480: epoch training perplexity 517.9443\n",
      "INFO:tensorflow:Epoch 17 (0.8621 complete), Iteration 490: epoch training perplexity 505.5433\n",
      "INFO:tensorflow:Epoch 18 (0.2069 complete), Iteration 500: epoch training perplexity 524.1622\n",
      "INFO:tensorflow:Epoch 18, Iteration 500: validation perplexity 2658.1034\n",
      "INFO:tensorflow:Epoch 18 (0.5517 complete), Iteration 510: epoch training perplexity 512.0393\n",
      "INFO:tensorflow:Epoch 18 (0.8966 complete), Iteration 520: epoch training perplexity 501.7558\n",
      "INFO:tensorflow:Epoch 19 (0.2414 complete), Iteration 530: epoch training perplexity 481.3935\n",
      "INFO:tensorflow:Epoch 19 (0.5862 complete), Iteration 540: epoch training perplexity 473.9368\n",
      "INFO:tensorflow:Epoch 19 (0.9310 complete), Iteration 550: epoch training perplexity 466.2096\n",
      "INFO:tensorflow:Epoch 20 (0.2759 complete), Iteration 560: epoch training perplexity 470.4682\n",
      "INFO:tensorflow:Epoch 20 (0.6207 complete), Iteration 570: epoch training perplexity 465.5394\n",
      "INFO:tensorflow:Epoch 20 (0.9655 complete), Iteration 580: epoch training perplexity 454.6177\n",
      "INFO:tensorflow:Epoch 21 (0.3103 complete), Iteration 590: epoch training perplexity 450.2346\n",
      "INFO:tensorflow:Epoch 21 (0.6552 complete), Iteration 600: epoch training perplexity 436.7374\n",
      "INFO:tensorflow:Epoch 21, Iteration 600: validation perplexity 3250.4603\n",
      "INFO:tensorflow:Epoch 22 (0.0000 complete), Iteration 610: epoch training perplexity 450.5698\n",
      "INFO:tensorflow:Epoch 22 (0.3448 complete), Iteration 620: epoch training perplexity 435.6256\n",
      "INFO:tensorflow:Epoch 22 (0.6897 complete), Iteration 630: epoch training perplexity 425.5261\n",
      "INFO:tensorflow:Epoch 23 (0.0345 complete), Iteration 640: epoch training perplexity 418.1402\n",
      "INFO:tensorflow:Epoch 23 (0.3793 complete), Iteration 650: epoch training perplexity 418.9880\n",
      "INFO:tensorflow:Epoch 23 (0.7241 complete), Iteration 660: epoch training perplexity 406.7319\n",
      "INFO:tensorflow:Epoch 24 (0.0690 complete), Iteration 670: epoch training perplexity 414.2936\n",
      "INFO:tensorflow:Epoch 24 (0.4138 complete), Iteration 680: epoch training perplexity 407.2065\n",
      "INFO:tensorflow:Epoch 24 (0.7586 complete), Iteration 690: epoch training perplexity 393.2586\n",
      "INFO:tensorflow:Epoch 25 (0.1034 complete), Iteration 700: epoch training perplexity 401.8457\n",
      "INFO:tensorflow:Epoch 25, Iteration 700: validation perplexity 3273.3907\n",
      "INFO:tensorflow:Epoch 25 (0.4483 complete), Iteration 710: epoch training perplexity 390.8416\n",
      "INFO:tensorflow:Epoch 25 (0.7931 complete), Iteration 720: epoch training perplexity 378.0247\n",
      "INFO:tensorflow:Stop training at epoch 26, iteration 725\n",
      "INFO:tensorflow:Saved model in model \n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree('model', ignore_errors = True)\n",
    "os.makedirs('model')\n",
    "shutil.rmtree('summary', ignore_errors = True)\n",
    "os.makedirs('summary')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    rnn.train(\n",
    "        session = sess, \n",
    "        training_factory = train_fact, \n",
    "        parameters = Parameters(learning_rate = 0.01, keep_probability = 0.5),\n",
    "        exit_criteria = ExitCriteria(max_iterations = None, max_epochs = 25), \n",
    "        validation = Validation(interval = 100, epoch_factory = valid_fact), \n",
    "        logging_interval = 10, \n",
    "        directories = Directories('model', 'summary')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small,\n",
    "        # let's give them a better error message\n",
    "        if self.num_batches == 0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1),\n",
    "                                  self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1),\n",
    "                                  self.num_batches, 1)\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('data', 16, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 20)\n"
     ]
    }
   ],
   "source": [
    "print(b[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
